{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensorflow Prophet Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of librairies\n",
    "import tensorflow as tf\n",
    "import mysql.connector as mariadb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "from joblib import load\n",
    "from timeloop import Timeloop\n",
    "from datetime import timedelta\n",
    "import time \n",
    "\n",
    "class sql_query:\n",
    "    def __init__(self, credentials_path):\n",
    "        self.db_credentials = pd.read_csv(credentials_path, index_col=\"Field\")\n",
    "      \n",
    "    \n",
    "    def __call__(self, query):\n",
    "        \n",
    "        mariadb_connection = mariadb.connect(\n",
    "            user=self.db_credentials.loc[\"user\"][0],\n",
    "            password=self.db_credentials.loc[\"password\"][0],\n",
    "            host=self.db_credentials.loc[\"host\"][0],\n",
    "            port=3306,\n",
    "            db = \"db_velib\")\n",
    "        \n",
    "        self.cursor = mariadb_connection.cursor()\n",
    "        cursor = self.cursor\n",
    "        cursor.execute(\"SET  time_zone = 'Europe/Paris'\")\n",
    "        cursor.execute(query)\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "        df = pd.DataFrame(cursor, columns=field_names)\n",
    "        return df\n",
    "    \n",
    "# Transforming the input data in the proper format \n",
    "\n",
    "\n",
    "def measure_rmse(actual, predicted):\n",
    "    return math.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "def list_stations():\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials - write.csv\")\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT station_id FROM velib_realtime\n",
    "    \"\"\"\n",
    "    df= request(query)\n",
    "    # Removing bad values\n",
    "    df= df.drop(0)\n",
    "    df = df.drop(1391)\n",
    "    list_of_stations = list(df.station_id)\n",
    "    return list_of_stations\n",
    "\n",
    "def loading_models_unique(station_id, day_of_testing):\n",
    "\n",
    "    try:\n",
    "        LSTM_A = tf.keras.models.load_model('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_A.h5'.format(day_of_testing, station_id))\n",
    "        LSTM_B = tf.keras.models.load_model('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_B.h5'.format(day_of_testing, station_id))\n",
    "        std = load('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - std.joblib'.format(day_of_testing, station_id))\n",
    "        return LSTM_A, LSTM_B, std\n",
    "    \n",
    "    except:\n",
    "        print('impossible to load ', list_of_stations[i])\n",
    "\n",
    "\n",
    "def create_result_df():\n",
    "    # Extracting base for prediction \n",
    "\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials - write.csv\")\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT station_id, date_of_update, nb_total_free_bikes FROM db_velib.velib_realtime\n",
    "    WHERE date_of_update >= DATE_SUB(NOW(), INTERVAL 185 Minute) AND MINUTE(date_of_update)%5 = 0\n",
    "    ORDER BY station_id, date_of_update ASC;\n",
    "    \"\"\"\n",
    "    df= request(query)\n",
    "    df.index = df.date_of_update\n",
    "    df = df[['station_id','nb_total_free_bikes']]\n",
    "    df = df.pivot_table(df, index= 'station_id', columns=df.index)\n",
    "\n",
    "    # Creating dataframe for proper predction\n",
    "\n",
    "    df_prediction = pd.DataFrame(index=df.index, columns=['last_observations','model_A', 'model_B', 'date_of_prediction'])\n",
    "    \n",
    "    for i in df_prediction.index:\n",
    "        df_prediction[\"last_observations\"].loc[i] = np.array(df.loc[i])\n",
    "    \n",
    "    df_prediction['date_of_prediction'] = str(pd.Timestamp.now())[:16]\n",
    "    return df_prediction\n",
    "\n",
    "\n",
    "def predict_iteration_unique(station_id, df_prediction, LSTM_A, LSTM_B, std):\n",
    "    # Request for each minutes\n",
    "\n",
    "    try:\n",
    "        input_data = std.transform(df_prediction[df_prediction.index == station_id][\"last_observations\"].iloc[0].reshape(-1, 1))[-36:]\n",
    "        df_prediction.loc[station_id]['model_A'] = std.inverse_transform(LSTM_A.predict(input_data.reshape(1,past_history,1))[0])\n",
    "        df_prediction.loc[station_id]['model_B'] = std.inverse_transform(LSTM_B.predict(input_data.reshape(1,past_history,1))[0])\n",
    "        return df_prediction\n",
    "    except:\n",
    "        print('error on ', station_id)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Basic Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1390/1390 [01:30<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 1.45 s, total: 1min 35s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Main pipelinhe\n",
    "\n",
    "# Variables\n",
    "day_of_testing = '2020-05-19'\n",
    "\n",
    "# Loading global model for al stations\n",
    "station_id = 'global'\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "list_of_stations = list_stations()\n",
    "df_prediction = create_result_df()\n",
    "\n",
    "# Loading the models only once\n",
    "LSTM_A, LSTM_B, std = loading_models_unique(station_id, day_of_testing)\n",
    "\n",
    "for station_id in tqdm(list_of_stations):\n",
    "\n",
    "    df_prediction = predict_iteration_unique(station_id, df_prediction, LSTM_A, LSTM_B, std)\n",
    "\n",
    "df_prediction.to_csv('prediction - {}.csv'.format(str(pd.Timestamp.now())[:16]))\n",
    "df_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_observations</th>\n",
       "      <th>model_A</th>\n",
       "      <th>model_B</th>\n",
       "      <th>date_of_prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>[14, 14, 15, 14, 14, 12, 13, 15, 15, 14, 13, 1...</td>\n",
       "      <td>[15.273676, 15.231695, 15.175198, 15.235238, 1...</td>\n",
       "      <td>[14.727909, 14.658568, 14.602522, 14.43373, 14...</td>\n",
       "      <td>2020-05-27 19:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>[21, 21, 21, 20, 20, 18, 18, 19, 20, 21, 20, 1...</td>\n",
       "      <td>[18.044127, 17.870247, 17.801634, 18.003893, 1...</td>\n",
       "      <td>[17.52366, 17.463417, 17.327711, 17.12547, 17....</td>\n",
       "      <td>2020-05-27 19:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>[41, 40, 38, 39, 38, 40, 40, 40, 39, 40, 40, 3...</td>\n",
       "      <td>[38.40056, 38.845818, 38.19857, 38.379547, 38....</td>\n",
       "      <td>[40.199287, 40.079327, 39.655148, 39.16375, 38...</td>\n",
       "      <td>2020-05-27 19:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>[32, 32, 32, 30, 30, 32, 29, 30, 31, 30, 31, 3...</td>\n",
       "      <td>[22.111727, 21.848858, 21.868937, 21.952255, 2...</td>\n",
       "      <td>[22.12679, 22.086313, 21.87791, 21.54091, 21.5...</td>\n",
       "      <td>2020-05-27 19:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>[16, 16, 16, 15, 16, 16, 16, 15, 15, 15, 15, 1...</td>\n",
       "      <td>[3.229335, 3.2705624, 3.3252575, 3.276993, 3.3...</td>\n",
       "      <td>[3.8500624, 3.9042127, 3.967713, 3.9145994, 3....</td>\n",
       "      <td>2020-05-27 19:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            last_observations  \\\n",
       "station_id                                                      \n",
       "1001        [14, 14, 15, 14, 14, 12, 13, 15, 15, 14, 13, 1...   \n",
       "1002        [21, 21, 21, 20, 20, 18, 18, 19, 20, 21, 20, 1...   \n",
       "1003        [41, 40, 38, 39, 38, 40, 40, 40, 39, 40, 40, 3...   \n",
       "1006        [32, 32, 32, 30, 30, 32, 29, 30, 31, 30, 31, 3...   \n",
       "1007        [16, 16, 16, 15, 16, 16, 16, 15, 15, 15, 15, 1...   \n",
       "\n",
       "                                                      model_A  \\\n",
       "station_id                                                      \n",
       "1001        [15.273676, 15.231695, 15.175198, 15.235238, 1...   \n",
       "1002        [18.044127, 17.870247, 17.801634, 18.003893, 1...   \n",
       "1003        [38.40056, 38.845818, 38.19857, 38.379547, 38....   \n",
       "1006        [22.111727, 21.848858, 21.868937, 21.952255, 2...   \n",
       "1007        [3.229335, 3.2705624, 3.3252575, 3.276993, 3.3...   \n",
       "\n",
       "                                                      model_B  \\\n",
       "station_id                                                      \n",
       "1001        [14.727909, 14.658568, 14.602522, 14.43373, 14...   \n",
       "1002        [17.52366, 17.463417, 17.327711, 17.12547, 17....   \n",
       "1003        [40.199287, 40.079327, 39.655148, 39.16375, 38...   \n",
       "1006        [22.12679, 22.086313, 21.87791, 21.54091, 21.5...   \n",
       "1007        [3.8500624, 3.9042127, 3.967713, 3.9145994, 3....   \n",
       "\n",
       "           date_of_prediction  \n",
       "station_id                     \n",
       "1001         2020-05-27 19:59  \n",
       "1002         2020-05-27 19:59  \n",
       "1003         2020-05-27 19:59  \n",
       "1006         2020-05-27 19:59  \n",
       "1007         2020-05-27 19:59  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Only One prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_iteration_unique(list_of_stations, df_prediction, LSTM_A, LSTM_B, std):\n",
    "    # Request for each minutes\n",
    "    for station_id in tqdm(list_of_stations):\n",
    "        try:\n",
    "            input_data = std.transform(df_prediction[df_prediction.index == station_id][\"last_observations\"].iloc[0].reshape(-1, 1))[-36:]\n",
    "            df_prediction.loc[station_id]['model_A'] = std.inverse_transform(LSTM_A.predict(input_data.reshape(1,past_history,1))[0])\n",
    "      #      df_prediction.loc[station_id]['model_B'] = std.inverse_transform(LSTM_B.predict(input_data.reshape(1,past_history,1))[0])\n",
    "            df_prediction\n",
    "        except:\n",
    "            print('error on ', station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1390/1390 [00:41<00:00, 33.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 43.6 s, sys: 797 ms, total: 44.4 s\n",
      "Wall time: 43 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_observations</th>\n",
       "      <th>model_A</th>\n",
       "      <th>model_B</th>\n",
       "      <th>date_of_prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>[2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 5, 5, 7, 9, ...</td>\n",
       "      <td>[13.166178, 13.274235, 13.22399, 13.162204, 13...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-28 11:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>[8, 8, 9, 10, 10, 7, 7, 9, 8, 10, 12, 12, 12, ...</td>\n",
       "      <td>[20.191088, 20.110025, 20.033934, 20.046398, 1...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-28 11:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>[9, 9, 9, 11, 11, 10, 9, 9, 10, 11, 14, 14, 14...</td>\n",
       "      <td>[30.405016, 30.602694, 30.363323, 30.177603, 3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-28 11:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>[24, 24, 25, 24, 24, 25, 25, 25, 24, 25, 26, 2...</td>\n",
       "      <td>[31.31989, 31.413927, 31.088177, 31.110306, 31...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-28 11:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>[9, 9, 9, 11, 11, 11, 12, 10, 10, 10, 11, 12, ...</td>\n",
       "      <td>[14.18355, 14.246721, 14.187556, 14.138256, 14...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-05-28 11:18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            last_observations  \\\n",
       "station_id                                                      \n",
       "1001        [2, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 5, 5, 7, 9, ...   \n",
       "1002        [8, 8, 9, 10, 10, 7, 7, 9, 8, 10, 12, 12, 12, ...   \n",
       "1003        [9, 9, 9, 11, 11, 10, 9, 9, 10, 11, 14, 14, 14...   \n",
       "1006        [24, 24, 25, 24, 24, 25, 25, 25, 24, 25, 26, 2...   \n",
       "1007        [9, 9, 9, 11, 11, 11, 12, 10, 10, 10, 11, 12, ...   \n",
       "\n",
       "                                                      model_A model_B  \\\n",
       "station_id                                                              \n",
       "1001        [13.166178, 13.274235, 13.22399, 13.162204, 13...     NaN   \n",
       "1002        [20.191088, 20.110025, 20.033934, 20.046398, 1...     NaN   \n",
       "1003        [30.405016, 30.602694, 30.363323, 30.177603, 3...     NaN   \n",
       "1006        [31.31989, 31.413927, 31.088177, 31.110306, 31...     NaN   \n",
       "1007        [14.18355, 14.246721, 14.187556, 14.138256, 14...     NaN   \n",
       "\n",
       "           date_of_prediction  \n",
       "station_id                     \n",
       "1001         2020-05-28 11:18  \n",
       "1002         2020-05-28 11:18  \n",
       "1003         2020-05-28 11:18  \n",
       "1006         2020-05-28 11:18  \n",
       "1007         2020-05-28 11:18  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Main pipelinhe\n",
    "\n",
    "# Variables\n",
    "day_of_testing = '2020-05-19'\n",
    "\n",
    "# Loading global model for al stations\n",
    "station_id = 'global'\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "list_of_stations = list_stations()\n",
    "df_prediction = create_result_df()\n",
    "\n",
    "# Loading the models only once\n",
    "LSTM_A, LSTM_B, std = loading_models_unique(station_id, day_of_testing)\n",
    "\n",
    "predict_iteration_unique(list_of_stations, df_prediction, LSTM_A, LSTM_B, std)\n",
    "\n",
    "df_prediction.to_csv('prediction - {}.csv'.format(str(pd.Timestamp.now())[:16]))\n",
    "df_prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_iteration_unique(list_of_stations, df_prediction, LSTM_A, LSTM_B, std):\n",
    "    # Request for each minutes\n",
    "    for station_id in tqdm(list_of_stations):\n",
    "        try:\n",
    "            input_data = std.transform(df_prediction[df_prediction.index == station_id][\"last_observations\"].iloc[0].reshape(-1, 1))[-36:]\n",
    "            df_prediction.loc[station_id]['model_A'] = std.inverse_transform(LSTM_A.predict(input_data.reshape(1,past_history,1))[0])\n",
    "      #      df_prediction.loc[station_id]['model_B'] = std.inverse_transform(LSTM_B.predict(input_data.reshape(1,past_history,1))[0])\n",
    "            df_prediction\n",
    "        except:\n",
    "            print('error on ', station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
      "CPU times: user 1.43 s, sys: 249 ms, total: 1.68 s\n",
      "Wall time: 2.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialisation\n",
    "\n",
    "# Variables\n",
    "day_of_testing = '2020-05-19'\n",
    "\n",
    "# Loading global model for al stations\n",
    "station_id = 'global'\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "list_of_stations = list_stations()\n",
    "df_prediction = create_result_df()\n",
    "\n",
    "# Loading the models only once\n",
    "LSTM_A, LSTM_B, std = loading_models_unique(station_id, day_of_testing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tl = Timeloop()\n",
    "\n",
    "@tl.job(interval=timedelta(minutes=5))\n",
    "def predicting_by_5_minutes():\n",
    "    df_prediction = create_result_df()\n",
    "    predict_iteration_unique(list_of_stations, df_prediction, LSTM_A, LSTM_B, std)\n",
    "    df_prediction.to_csv('prediction - {}.csv'.format(str(pd.Timestamp.now())[:16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-28 11:23:53,079] [timeloop] [INFO] Starting Timeloop..\n",
      "[2020-05-28 11:23:53,081] [timeloop] [INFO] Registered job <function predicting_by_5_minutes at 0x7f1fac0d5560>\n",
      "[2020-05-28 11:23:53,082] [timeloop] [INFO] Timeloop now started. Jobs will run based on the interval set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1390/1390 [00:42<00:00, 32.77it/s]\n",
      "100%|██████████| 1390/1390 [00:40<00:00, 34.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-05-28 11:36:52,998] [timeloop] [INFO] Stopping job <function predicting_by_5_minutes at 0x7f1fac0d5560>\n",
      "[2020-05-28 11:36:53,000] [timeloop] [INFO] Timeloop exited.\n"
     ]
    }
   ],
   "source": [
    "#tl.start(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Consolidation for script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of librairies\n",
    "import tensorflow as tf\n",
    "import mysql.connector as mariadb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "from joblib import load\n",
    "from timeloop import Timeloop\n",
    "from datetime import timedelta\n",
    "import time \n",
    "\n",
    "class sql_query:\n",
    "    def __init__(self, credentials_path):\n",
    "        self.db_credentials = pd.read_csv(credentials_path, index_col=\"Field\")\n",
    "      \n",
    "    \n",
    "    def __call__(self, query):\n",
    "        \n",
    "        mariadb_connection = mariadb.connect(\n",
    "            user=self.db_credentials.loc[\"user\"][0],\n",
    "            password=self.db_credentials.loc[\"password\"][0],\n",
    "            host=self.db_credentials.loc[\"host\"][0],\n",
    "            port=3306,\n",
    "            db = \"db_velib\")\n",
    "        \n",
    "        self.cursor = mariadb_connection.cursor()\n",
    "        cursor = self.cursor\n",
    "        cursor.execute(\"SET  time_zone = 'Europe/Paris'\")\n",
    "        cursor.execute(query)\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "        df = pd.DataFrame(cursor, columns=field_names)\n",
    "        return df\n",
    "    \n",
    "# Transforming the input data in the proper format \n",
    "\n",
    "\n",
    "def measure_rmse(actual, predicted):\n",
    "    return math.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "def list_stations():\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials - write.csv\")\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT station_id FROM velib_realtime\n",
    "    \"\"\"\n",
    "    df= request(query)\n",
    "    # Removing bad values\n",
    "    df= df.drop(0)\n",
    "    df = df.drop(1391)\n",
    "    list_of_stations = list(df.station_id)\n",
    "    return list_of_stations\n",
    "\n",
    "def loading_models_unique(station_id, day_of_testing):\n",
    "\n",
    "    try:\n",
    "        LSTM_A = tf.keras.models.load_model('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_A.h5'.format(day_of_testing, station_id))\n",
    "        LSTM_B = tf.keras.models.load_model('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_B.h5'.format(day_of_testing, station_id))\n",
    "        std = load('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - std.joblib'.format(day_of_testing, station_id))\n",
    "        return LSTM_A, LSTM_B, std\n",
    "    \n",
    "    except:\n",
    "        print('impossible to load ', list_of_stations[i])\n",
    "\n",
    "\n",
    "def create_result_df():\n",
    "    # Extracting base for prediction \n",
    "\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials - write.csv\")\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT station_id, date_of_update, nb_total_free_bikes FROM db_velib.velib_realtime\n",
    "    WHERE date_of_update >= DATE_SUB(NOW(), INTERVAL 185 Minute) AND MINUTE(date_of_update)%5 = 0\n",
    "    ORDER BY station_id, date_of_update ASC;\n",
    "    \"\"\"\n",
    "    df= request(query)\n",
    "    df.index = df.date_of_update\n",
    "    df = df[['station_id','nb_total_free_bikes']]\n",
    "    df = df.pivot_table(df, index= 'station_id', columns=df.index)\n",
    "\n",
    "    # Creating dataframe for proper predction\n",
    "\n",
    "    df_prediction = pd.DataFrame(index=df.index, columns=['last_observations','model_A', 'model_B', 'date_of_prediction'])\n",
    "    \n",
    "    for i in df_prediction.index:\n",
    "        df_prediction[\"last_observations\"].loc[i] = np.array(df.loc[i])\n",
    "    \n",
    "    df_prediction['date_of_prediction'] = str(pd.Timestamp.now())[:16]\n",
    "    return df_prediction\n",
    "\n",
    "\n",
    "def predict_iteration_unique(list_of_stations, df_prediction, LSTM_A, LSTM_B, std):\n",
    "    # Request for each minutes\n",
    "    for station_id in tqdm(list_of_stations):\n",
    "        try:\n",
    "            input_data = std.transform(df_prediction[df_prediction.index == station_id][\"last_observations\"].iloc[0].reshape(-1, 1))[-36:]\n",
    "            df_prediction.loc[station_id]['model_A'] = std.inverse_transform(LSTM_A.predict(input_data.reshape(1,past_history,1))[0])\n",
    "      #      df_prediction.loc[station_id]['model_B'] = std.inverse_transform(LSTM_B.predict(input_data.reshape(1,past_history,1))[0])\n",
    "            df_prediction\n",
    "        except:\n",
    "            print('error on ', station_id)\n",
    "            \n",
    "%%time\n",
    "# Initialisation\n",
    "\n",
    "# Variables\n",
    "day_of_testing = '2020-05-19'\n",
    "\n",
    "# Loading global model for al stations\n",
    "station_id = 'global'\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "list_of_stations = list_stations()\n",
    "\n",
    "# Loading the models only once\n",
    "LSTM_A, LSTM_B, std = loading_models_unique(station_id, day_of_testing)\n",
    "\n",
    "tl = Timeloop()\n",
    "\n",
    "@tl.job(interval=timedelta(minutes=1))\n",
    "def predicting_by_5_minutes():\n",
    "    df_prediction = create_result_df()\n",
    "    predict_iteration_unique(list_of_stations, df_prediction, LSTM_A, LSTM_B, std)\n",
    "    df_prediction.to_csv('prediction - {}.csv'.format(str(pd.Timestamp.now())[:16]))\n",
    "    \n",
    "tl.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
