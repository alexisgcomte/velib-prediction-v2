{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensorflow Prophet Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of librairies\n",
    "import tensorflow as tf\n",
    "import mysql.connector as mariadb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "from joblib import load\n",
    "from timeloop import Timeloop\n",
    "from datetime import timedelta\n",
    "import time \n",
    "\n",
    "class sql_query:\n",
    "    def __init__(self, credentials_path):\n",
    "        self.db_credentials = pd.read_csv(credentials_path, index_col=\"Field\")\n",
    "      \n",
    "    \n",
    "    def __call__(self, query):\n",
    "        \n",
    "        mariadb_connection = mariadb.connect(\n",
    "            user=self.db_credentials.loc[\"user\"][0],\n",
    "            password=self.db_credentials.loc[\"password\"][0],\n",
    "            host=self.db_credentials.loc[\"host\"][0],\n",
    "            port=3306,\n",
    "            db = \"db_velib\")\n",
    "        \n",
    "        self.cursor = mariadb_connection.cursor()\n",
    "        cursor = self.cursor\n",
    "        cursor.execute(\"SET  time_zone = 'Europe/Paris'\")\n",
    "        cursor.execute(query)\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "        df = pd.DataFrame(cursor, columns=field_names)\n",
    "        return df\n",
    "    \n",
    "# Transforming the input data in the proper format \n",
    "\n",
    "\n",
    "def measure_rmse(actual, predicted):\n",
    "    return math.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "def list_stations():\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials - write.csv\")\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT station_id FROM velib_realtime\n",
    "    \"\"\"\n",
    "    df= request(query)\n",
    "    # Removing bad values\n",
    "    df= df.drop(0)\n",
    "    df = df.drop(1391)\n",
    "    list_of_stations = list(df.station_id)\n",
    "    return list_of_stations\n",
    "\n",
    "def loading_models_unique(station_id, day_of_testing):\n",
    "\n",
    "    try:\n",
    "        LSTM_A = tf.keras.models.load_model('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_A.h5'.format(day_of_testing, station_id))\n",
    "        LSTM_B = tf.keras.models.load_model('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_B.h5'.format(day_of_testing, station_id))\n",
    "        std = load('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - std.joblib'.format(day_of_testing, station_id))\n",
    "        return LSTM_A, LSTM_B, std\n",
    "    \n",
    "    except:\n",
    "        print('impossible to load ', list_of_stations[i])\n",
    "\n",
    "\n",
    "def create_result_df():\n",
    "    # Extracting base for prediction \n",
    "\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials - write.csv\")\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT station_id, date_of_update, nb_total_free_bikes FROM db_velib.velib_realtime\n",
    "    WHERE date_of_update >= DATE_SUB(NOW(), INTERVAL 185 Minute) AND MINUTE(date_of_update)%5 = 0\n",
    "    ORDER BY station_id, date_of_update ASC;\n",
    "    \"\"\"\n",
    "    df= request(query)\n",
    "    df.index = df.date_of_update\n",
    "    df = df[['station_id','nb_total_free_bikes']]\n",
    "    df = df.pivot_table(df, index= 'station_id', columns=df.index)\n",
    "\n",
    "    # Creating dataframe for proper predction\n",
    "\n",
    "    df_prediction = pd.DataFrame(index=df.index, columns=['last_observations','model_A', 'model_B', 'date_of_prediction'])\n",
    "    \n",
    "    for i in df_prediction.index:\n",
    "        df_prediction[\"last_observations\"].loc[i] = np.array(df.loc[i])\n",
    "    \n",
    "    df_prediction['date_of_prediction'] = str(pd.Timestamp.now())[:16]\n",
    "    return df_prediction\n",
    "\n",
    "\n",
    "def predict_iteration_unique(station_id, df_prediction, LSTM_A, LSTM_B, std):\n",
    "    # Request for each minutes\n",
    "\n",
    "    try:\n",
    "        input_data = std.transform(df_prediction[df_prediction.index == station_id][\"last_observations\"].iloc[0].reshape(-1, 1))[-36:]\n",
    "        df_prediction.loc[station_id]['model_A'] = std.inverse_transform(LSTM_A.predict(input_data.reshape(1,past_history,1))[0])\n",
    "        df_prediction.loc[station_id]['model_B'] = std.inverse_transform(LSTM_B.predict(input_data.reshape(1,past_history,1))[0])\n",
    "        return df_prediction\n",
    "    except:\n",
    "        print('error on ', station_id)\n",
    "            \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Basic Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1390/1390 [01:30<00:00, 15.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 34s, sys: 1.45 s, total: 1min 35s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Main pipelinhe\n",
    "\n",
    "# Variables\n",
    "day_of_testing = '2020-05-19'\n",
    "\n",
    "# Loading global model for al stations\n",
    "station_id = 'global'\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "list_of_stations = list_stations()\n",
    "df_prediction = create_result_df()\n",
    "\n",
    "# Loading the models only once\n",
    "LSTM_A, LSTM_B, std = loading_models_unique(station_id, day_of_testing)\n",
    "\n",
    "for station_id in tqdm(list_of_stations):\n",
    "\n",
    "    df_prediction = predict_iteration_unique(station_id, df_prediction, LSTM_A, LSTM_B, std)\n",
    "\n",
    "df_prediction.to_csv('prediction - {}.csv'.format(str(pd.Timestamp.now())[:16]))\n",
    "df_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>last_observations</th>\n",
       "      <th>model_A</th>\n",
       "      <th>model_B</th>\n",
       "      <th>date_of_prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>[14, 14, 15, 14, 14, 12, 13, 15, 15, 14, 13, 1...</td>\n",
       "      <td>[15.273676, 15.231695, 15.175198, 15.235238, 1...</td>\n",
       "      <td>[14.727909, 14.658568, 14.602522, 14.43373, 14...</td>\n",
       "      <td>2020-05-27 19:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>[21, 21, 21, 20, 20, 18, 18, 19, 20, 21, 20, 1...</td>\n",
       "      <td>[18.044127, 17.870247, 17.801634, 18.003893, 1...</td>\n",
       "      <td>[17.52366, 17.463417, 17.327711, 17.12547, 17....</td>\n",
       "      <td>2020-05-27 19:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>[41, 40, 38, 39, 38, 40, 40, 40, 39, 40, 40, 3...</td>\n",
       "      <td>[38.40056, 38.845818, 38.19857, 38.379547, 38....</td>\n",
       "      <td>[40.199287, 40.079327, 39.655148, 39.16375, 38...</td>\n",
       "      <td>2020-05-27 19:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>[32, 32, 32, 30, 30, 32, 29, 30, 31, 30, 31, 3...</td>\n",
       "      <td>[22.111727, 21.848858, 21.868937, 21.952255, 2...</td>\n",
       "      <td>[22.12679, 22.086313, 21.87791, 21.54091, 21.5...</td>\n",
       "      <td>2020-05-27 19:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>[16, 16, 16, 15, 16, 16, 16, 15, 15, 15, 15, 1...</td>\n",
       "      <td>[3.229335, 3.2705624, 3.3252575, 3.276993, 3.3...</td>\n",
       "      <td>[3.8500624, 3.9042127, 3.967713, 3.9145994, 3....</td>\n",
       "      <td>2020-05-27 19:59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            last_observations  \\\n",
       "station_id                                                      \n",
       "1001        [14, 14, 15, 14, 14, 12, 13, 15, 15, 14, 13, 1...   \n",
       "1002        [21, 21, 21, 20, 20, 18, 18, 19, 20, 21, 20, 1...   \n",
       "1003        [41, 40, 38, 39, 38, 40, 40, 40, 39, 40, 40, 3...   \n",
       "1006        [32, 32, 32, 30, 30, 32, 29, 30, 31, 30, 31, 3...   \n",
       "1007        [16, 16, 16, 15, 16, 16, 16, 15, 15, 15, 15, 1...   \n",
       "\n",
       "                                                      model_A  \\\n",
       "station_id                                                      \n",
       "1001        [15.273676, 15.231695, 15.175198, 15.235238, 1...   \n",
       "1002        [18.044127, 17.870247, 17.801634, 18.003893, 1...   \n",
       "1003        [38.40056, 38.845818, 38.19857, 38.379547, 38....   \n",
       "1006        [22.111727, 21.848858, 21.868937, 21.952255, 2...   \n",
       "1007        [3.229335, 3.2705624, 3.3252575, 3.276993, 3.3...   \n",
       "\n",
       "                                                      model_B  \\\n",
       "station_id                                                      \n",
       "1001        [14.727909, 14.658568, 14.602522, 14.43373, 14...   \n",
       "1002        [17.52366, 17.463417, 17.327711, 17.12547, 17....   \n",
       "1003        [40.199287, 40.079327, 39.655148, 39.16375, 38...   \n",
       "1006        [22.12679, 22.086313, 21.87791, 21.54091, 21.5...   \n",
       "1007        [3.8500624, 3.9042127, 3.967713, 3.9145994, 3....   \n",
       "\n",
       "           date_of_prediction  \n",
       "station_id                     \n",
       "1001         2020-05-27 19:59  \n",
       "1002         2020-05-27 19:59  \n",
       "1003         2020-05-27 19:59  \n",
       "1006         2020-05-27 19:59  \n",
       "1007         2020-05-27 19:59  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis cuda and processor, only one CPU core is predicting... can we use multithread?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from multiprocessing import freeze_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
      "CPU times: user 1.2 s, sys: 48.1 ms, total: 1.25 s\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Main pipelinhe\n",
    "\n",
    "# Variables\n",
    "day_of_testing = '2020-05-19'\n",
    "\n",
    "# Loading global model for al stations\n",
    "station_id = 'global'\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "list_of_stations = list_stations()[:15]\n",
    "df_prediction = create_result_df()\n",
    "\n",
    "# Loading the models only once\n",
    "LSTM_A, LSTM_B, std = loading_models_unique(station_id, day_of_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class df_prediction_generation_class:\n",
    "    def __init__(self, df_prediction, LSTM_A, LSTM_B, std):\n",
    "        self.df_prediction = df_prediction\n",
    "        self.LSTM_A = LSTM_A\n",
    "        self.LSTM_B = LSTM_B\n",
    "        self.std = std\n",
    "      \n",
    "    def __call__(self, station_id):\n",
    "        df_prediction = predict_iteration_unique(station_id, df_prediction, LSTM_A, LSTM_B, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction_generation_temp = df_prediction_generation_class(df_prediction, LSTM_A, LSTM_B, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multiprocessing(func, i, n_processors):\n",
    "    with Pool(processes=n_processors) as pool:\n",
    "        return pool.map(func, i)\n",
    "\n",
    "def main(station_id):\n",
    "    n_processors = 14\n",
    "    out = run_multiprocessing(df_prediction_generation_temp, station_id, n_processors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't pickle _thread.RLock objects",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-6b1f838edf18>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(station_id)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstation_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mn_processors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_multiprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_prediction_generation_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstation_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_processors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-6b1f838edf18>\u001b[0m in \u001b[0;36mrun_multiprocessing\u001b[0;34m(func, i, n_processors)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_multiprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_processors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_processors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstation_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflowenv/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflowenv/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflowenv/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36m_handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    429\u001b[0m                         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m                         \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflowenv/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflowenv/lib/python3.7/multiprocessing/reduction.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle _thread.RLock objects"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_prediction = create_result_df()\n",
    "if __name__ == \"__main__\":\n",
    "    freeze_support()   # required to use multiprocessing\n",
    "    main(list_of_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Looping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_prediction.to_csv('prediction - {}.csv'.format(str(pd.Timestamp.now())[:16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "tl = Timeloop()\n",
    "\n",
    "\n",
    "@tl.job(interval=timedelta(seconds=30))\n",
    "def predicting_by_5_minutes():\n",
    "    df_prediction = predict_iteration()\n",
    "    df_prediction.to_csv('prediction - {}.csv'.format(str(pd.Timestamp.now())[:16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tl.start(block=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
