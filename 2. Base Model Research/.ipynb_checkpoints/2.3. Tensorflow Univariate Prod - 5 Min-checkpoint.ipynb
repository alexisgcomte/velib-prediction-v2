{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensorflow Prophet Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of librairies\n",
    "import tensorflow as tf\n",
    "import mysql.connector as mariadb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "class sql_query:\n",
    "    def __init__(self, credentials_path):\n",
    "        self.db_credentials = pd.read_csv(credentials_path, index_col=\"Field\")\n",
    "      \n",
    "    \n",
    "    def __call__(self, query):\n",
    "        \n",
    "        mariadb_connection = mariadb.connect(\n",
    "            user=self.db_credentials.loc[\"user\"][0],\n",
    "            password=self.db_credentials.loc[\"password\"][0],\n",
    "            host=self.db_credentials.loc[\"host\"][0],\n",
    "            port=3306,\n",
    "            db = \"db_velib\")\n",
    "        \n",
    "        self.cursor = mariadb_connection.cursor()\n",
    "    \n",
    "        cursor = self.cursor\n",
    "        cursor.execute(query)\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "        df = pd.DataFrame(cursor, columns=field_names)\n",
    "        return df\n",
    "    \n",
    "# Transforming the input data in the proper format \n",
    "\n",
    "\n",
    "\n",
    "def data_preparation(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "def measure_rmse(actual, predicted):\n",
    "    return math.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "def model_training(station_id, day_of_testing, past_history, future_target):\n",
    "\n",
    "\n",
    "    tf.random.set_seed(13)\n",
    "    past_history = 36\n",
    "    future_target = 6\n",
    "    STEP = 1\n",
    "    BATCH_SIZE = 32\n",
    "    BUFFER_SIZE = 100000\n",
    "    EPOCHS = 6\n",
    "    EVALUATION_INTERVAL = 200\n",
    "\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "    WHERE station_id = {}\n",
    "    AND date_of_update > DATE('2020-05-05')\n",
    "    AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "    AND MINUTE(date_of_update)%5=0\n",
    "    ORDER BY date_of_update ASC\n",
    "    \"\"\".format(station_id, day_of_testing)\n",
    "\n",
    "    df = request(query)\n",
    "    df.index = df['date_of_update']\n",
    "    df = df.nb_total_free_bikes\n",
    "    \n",
    "    \n",
    "    \n",
    "    TRAIN_SPLIT = round(df.shape[0]*0.7)\n",
    "\n",
    "    # StandardScaler transformation of the dataset\n",
    "\n",
    "    std = StandardScaler()\n",
    "    std.fit(df[:TRAIN_SPLIT].values.reshape(-1,1))\n",
    "    df = std.transform(df.values.reshape(-1,1))\n",
    "\n",
    "    # Creating proper format data\n",
    "\n",
    "    x_train, y_train = data_preparation(df, df[1:], 0, TRAIN_SPLIT,\n",
    "                                               past_history,\n",
    "                                               future_target, STEP)\n",
    "    x_val, y_val = data_preparation(df, df[1:], TRAIN_SPLIT, None,\n",
    "                                           past_history,\n",
    "                                           future_target, STEP)\n",
    "\n",
    "    # Creating format for NN intput\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "    x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)\n",
    "\n",
    "    # Creating batches for tensorflow use\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    val_data = val_data.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    # Modeling A\n",
    "    \n",
    "    LSTM_model_A = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(32, input_shape=x_train.shape[-2:]),\n",
    "        tf.keras.layers.Dense(future_target)\n",
    "    ])\n",
    "\n",
    "    LSTM_model_A.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    LSTM_model_A_history = LSTM_model_A.fit(train_data, epochs=EPOCHS,\n",
    "                                                steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                                validation_data=val_data,\n",
    "                                                validation_steps=200)\n",
    "    \n",
    "    # Modeling B\n",
    "\n",
    "    LSTM_model_B = keras.Sequential()\n",
    "    LSTM_model_B.add(\n",
    "      keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(\n",
    "          units=64,\n",
    "          input_shape=(x_train.shape[-2:])\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    LSTM_model_B.add(keras.layers.Dropout(rate=0.2))\n",
    "    LSTM_model_B.add(keras.layers.Dense(units=future_target))\n",
    "    LSTM_model_B.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    LSTM_model_B_history = LSTM_model_B.fit(train_data, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data,\n",
    "                                            validation_steps=200)\n",
    "    \n",
    "    \n",
    "    return LSTM_model_A, LSTM_model_A_history, LSTM_model_B, LSTM_model_B_history, std\n",
    "\n",
    "def results_filling(df_results):\n",
    "    \n",
    "    for i in df_results.index:\n",
    "        \n",
    "        try:\n",
    "            # interval // Taking the last 180 values\n",
    "            past_for_prediction = df[(i - datetime.timedelta(minutes=past_history*5+60)):i][-past_history:].values\n",
    "            past_for_prediction_encoded = std.transform(past_for_prediction.reshape(-1, 1))\n",
    "\n",
    "            # Prediction of A\n",
    "            results_A = LSTM_model_A.predict(past_for_prediction_encoded.reshape(1,past_history,1))[0]\n",
    "            results_A = std.inverse_transform(results_A)\n",
    "\n",
    "            # Prediction of B\n",
    "            results_B = LSTM_model_B.predict(past_for_prediction_encoded.reshape(1,past_history,1))[0]\n",
    "            results_B = std.inverse_transform(results_B)\n",
    "\n",
    "            df_results.prediction_A[i] = results_A\n",
    "            df_results.prediction_B[i] = results_B\n",
    "            df_results.real_values[i] = df[i: i + datetime.timedelta(minutes=60)][0:future_target].values\n",
    "\n",
    "            df_results.loc[i].metrics_A = measure_rmse(df_results.loc[i].real_values, df_results.loc[i].prediction_A)\n",
    "            df_results.loc[i].metrics_B = measure_rmse(df_results.loc[i].real_values, df_results.loc[i].prediction_B)\n",
    "        except:\n",
    "            print('error at', i)\n",
    "            \n",
    "            df_results.loc[i].metrics_A = None\n",
    "            df_results.loc[i].metrics_B = None\n",
    "        \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1001, 1002, 1003, 1006, 1007]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the list of the stations\n",
    "\n",
    "request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT station_id FROM velib_realtime\n",
    "\"\"\"\n",
    "df= request(query)\n",
    "# Removing bad values\n",
    "df= df.drop(0)\n",
    "df = df.drop(1391)\n",
    "list_of_stations = list(df.station_id)\n",
    "print(list_of_stations[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1390 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 3s 16ms/step - loss: 0.3140 - val_loss: 0.5406\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1808 - val_loss: 0.5044\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1604 - val_loss: 0.4638\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1456 - val_loss: 0.6738\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1405 - val_loss: 0.6636\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1324 - val_loss: 0.7203\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1253 - val_loss: 0.6942\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1157 - val_loss: 0.7865\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1048 - val_loss: 0.8394\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0924 - val_loss: 0.6171\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.3002 - val_loss: 0.3920\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1868 - val_loss: 0.3694\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1692 - val_loss: 0.5242\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1538 - val_loss: 0.6136\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1424 - val_loss: 0.7382\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1352 - val_loss: 0.8193\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1283 - val_loss: 0.6484\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1206 - val_loss: 0.8859\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1099 - val_loss: 0.7645\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1001 - val_loss: 0.6832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/1390 [00:42<16:27:29, 42.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished  1001\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.2439 - val_loss: 0.3547\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1250 - val_loss: 0.3158\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1133 - val_loss: 0.3152\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1083 - val_loss: 0.3144\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1028 - val_loss: 0.3120\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0980 - val_loss: 0.3189\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0988 - val_loss: 0.3345\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0945 - val_loss: 0.3348\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0943 - val_loss: 0.3433\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0896 - val_loss: 0.3305\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.2405 - val_loss: 0.3658\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1326 - val_loss: 0.3516\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1180 - val_loss: 0.3544\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1124 - val_loss: 0.3236\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1101 - val_loss: 0.3188\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1026 - val_loss: 0.3191\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1035 - val_loss: 0.3383\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0964 - val_loss: 0.3027\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0988 - val_loss: 0.3054\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0956 - val_loss: 0.3183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/1390 [01:24<16:20:09, 42.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished  1002\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.2088 - val_loss: 0.0362\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0821 - val_loss: 0.0365\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0788 - val_loss: 0.0338\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0756 - val_loss: 0.0349\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0723 - val_loss: 0.0354\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0708 - val_loss: 0.0348\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0678 - val_loss: 0.0346\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0670 - val_loss: 0.0343\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0664 - val_loss: 0.0374\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0644 - val_loss: 0.0363\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.1895 - val_loss: 0.0387\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0982 - val_loss: 0.0380\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0911 - val_loss: 0.0339\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0846 - val_loss: 0.0333\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0803 - val_loss: 0.0341\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0785 - val_loss: 0.0325\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0767 - val_loss: 0.0317\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0739 - val_loss: 0.0316\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0738 - val_loss: 0.0313\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0711 - val_loss: 0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/1390 [02:06<16:18:23, 42.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished  1003\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.2764 - val_loss: 0.2107\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1475 - val_loss: 0.2215\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1388 - val_loss: 0.1668\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1264 - val_loss: 0.1734\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1295 - val_loss: 0.1394\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1216 - val_loss: 0.1119\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1173 - val_loss: 0.1239\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1119 - val_loss: 0.1191\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1114 - val_loss: 0.1643\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1018 - val_loss: 0.1395\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.2668 - val_loss: 0.2197\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1615 - val_loss: 0.1582\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1437 - val_loss: 0.1086\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1406 - val_loss: 0.1331\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1315 - val_loss: 0.0884\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1256 - val_loss: 0.0796\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1254 - val_loss: 0.0846\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1165 - val_loss: 0.1112\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1155 - val_loss: 0.1205\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1138 - val_loss: 0.1507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 4/1390 [02:48<16:13:39, 42.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished  1006\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.1982 - val_loss: 0.1161\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0928 - val_loss: 0.0849\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0867 - val_loss: 0.0807\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0820 - val_loss: 0.0714\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0767 - val_loss: 0.0709\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0727 - val_loss: 0.0813\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0658 - val_loss: 0.0797\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0658 - val_loss: 0.0984\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0620 - val_loss: 0.1156\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0578 - val_loss: 0.1245\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.1931 - val_loss: 0.1061\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1068 - val_loss: 0.0963\n",
      "Epoch 3/10\n",
      "153/200 [=====================>........] - ETA: 0s - loss: 0.0950"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1390 [03:11<18:23:46, 47.78s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-29c7f2f2f3fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mLSTM_model_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM_model_A_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM_model_B\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM_model_B_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstation_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mday_of_testing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# importing results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-a51e75e4e2bc>\u001b[0m in \u001b[0;36mmodel_training\u001b[0;34m(station_id, day_of_testing, past_history, future_target)\u001b[0m\n\u001b[1;32m    154\u001b[0m                                             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEVALUATION_INTERVAL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                                             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                                             validation_steps=200)\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflowenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflowenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflowenv/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mdata_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_outs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         batch_outs = (batch_outs['total_loss'] + batch_outs['output_losses']\n\u001b[1;32m    162\u001b[0m                       + batch_outs['metrics'])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main pipelinhe\n",
    "\n",
    "# Variables\n",
    "day_of_testing = '2020-05-10'\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "\n",
    "# Request for database\n",
    "\n",
    "for station_id in tqdm(list_of_stations):\n",
    "    \n",
    "    request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "    WHERE station_id = {}\n",
    "    AND date_of_update > DATE('2020-05-05')\n",
    "    AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "    ORDER BY date_of_update ASC\n",
    "    \"\"\".format(station_id, day_of_testing)\n",
    "\n",
    "    df = request(query)\n",
    "    df.index = df['date_of_update']\n",
    "    df = df.nb_total_free_bikes\n",
    "    \n",
    "\n",
    "\n",
    "    df_results = pd.DataFrame(columns=['prediction_A', 'prediction_B', 'real_values', 'metrics_A', 'metrics_B'], index=pd.date_range(day_of_testing+' 06:00:00', periods=64, freq='15Min'))\n",
    "\n",
    "    # Training\n",
    "\n",
    "    LSTM_model_A, LSTM_model_A_history, LSTM_model_B, LSTM_model_B_history, std = model_training(station_id, day_of_testing, past_history, future_target)\n",
    "\n",
    "    # importing results\n",
    "    results_filling(df_results)\n",
    "    \n",
    "    df_results.to_csv(\"/home/exalis/Github/velib-prediction-v2/3. Results/2. Tensorflow Univariate/Tensorflow Univariate Results - {} - {}.csv\".format(day_of_testing, station_id))\n",
    "    \n",
    "    print('finished ', station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
