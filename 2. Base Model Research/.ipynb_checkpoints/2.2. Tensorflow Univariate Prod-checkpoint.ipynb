{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensorflow Prophet Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of librairies\n",
    "import tensorflow as tf\n",
    "import mysql.connector as mariadb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "class sql_query:\n",
    "    def __init__(self, credentials_path):\n",
    "        self.db_credentials = pd.read_csv(credentials_path, index_col=\"Field\")\n",
    "      \n",
    "    \n",
    "    def __call__(self, query):\n",
    "        \n",
    "        mariadb_connection = mariadb.connect(\n",
    "            user=self.db_credentials.loc[\"user\"][0],\n",
    "            password=self.db_credentials.loc[\"password\"][0],\n",
    "            host=self.db_credentials.loc[\"host\"][0],\n",
    "            port=3306,\n",
    "            db = \"db_velib\")\n",
    "        \n",
    "        self.cursor = mariadb_connection.cursor()\n",
    "    \n",
    "        cursor = self.cursor\n",
    "        cursor.execute(query)\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "        df = pd.DataFrame(cursor, columns=field_names)\n",
    "        return df\n",
    "    \n",
    "# Transforming the input data in the proper format \n",
    "\n",
    "def data_preparation(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "def measure_rmse(actual, predicted):\n",
    "    return math.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "\n",
    "def model_training(station_id, day_of_testing, past_history, future_target):\n",
    "\n",
    "\n",
    "    tf.random.set_seed(13)\n",
    "    past_history = 180\n",
    "    future_target = 30\n",
    "    STEP = 1\n",
    "    BATCH_SIZE = 32\n",
    "    BUFFER_SIZE = 100000\n",
    "    EPOCHS = 15\n",
    "    EVALUATION_INTERVAL = 200\n",
    "\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "\n",
    "    # Taking data from  station 9034 - Madeleine\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "    WHERE station_id = {}\n",
    "    AND date_of_update > DATE('2020-05-05')\n",
    "    AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "    ORDER BY date_of_update ASC\n",
    "    \"\"\".format(station_id, day_of_testing)\n",
    "\n",
    "    df = request(query)\n",
    "    df.index = df['date_of_update']\n",
    "    df = df.nb_total_free_bikes\n",
    "    \n",
    "    TRAIN_SPLIT = round(df.shape[0]*0.7)\n",
    "\n",
    "    # StandardScaler transformation of the dataset\n",
    "\n",
    "    std = StandardScaler()\n",
    "    std.fit(df[:TRAIN_SPLIT].values.reshape(-1,1))\n",
    "    df = std.transform(df.values.reshape(-1,1))\n",
    "\n",
    "    # Creating proper format data\n",
    "\n",
    "    x_train, y_train = data_preparation(df, df[1:], 0, TRAIN_SPLIT,\n",
    "                                               past_history,\n",
    "                                               future_target, STEP)\n",
    "    x_val, y_val = data_preparation(df, df[1:], TRAIN_SPLIT, None,\n",
    "                                           past_history,\n",
    "                                           future_target, STEP)\n",
    "\n",
    "    # Creating format for NN intput\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "    x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)\n",
    "\n",
    "    # Creating batches for tensorflow use\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    val_data = val_data.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    # Modeling A\n",
    "    \n",
    "    LSTM_model_A = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(32, input_shape=x_train.shape[-2:]),\n",
    "        tf.keras.layers.Dense(future_target)\n",
    "    ])\n",
    "\n",
    "    LSTM_model_A.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    LSTM_model_A_history = LSTM_model_A.fit(train_data, epochs=EPOCHS,\n",
    "                                                steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                                validation_data=val_data,\n",
    "                                                validation_steps=200)\n",
    "    \n",
    "    # Modeling B\n",
    "    \n",
    "    LSTM_model_B = keras.Sequential()\n",
    "    LSTM_model_B.add(\n",
    "      keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(\n",
    "          units=64,\n",
    "          input_shape=(x_train.shape[-2:])\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    LSTM_model_B.add(keras.layers.Dropout(rate=0.2))\n",
    "    LSTM_model_B.add(keras.layers.Dense(units=30))\n",
    "    LSTM_model_B.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    LSTM_model_B_history = LSTM_model_B.fit(train_data, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data,\n",
    "                                            validation_steps=200)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return LSTM_model_A, LSTM_model_A_history, LSTM_model_B, LSTM_model_B_history, std\n",
    "\n",
    "def results_filling(df_results):\n",
    "\n",
    "    for i in df_results.index:\n",
    "        try:\n",
    "            # interval // Taking the last 180 values\n",
    "            past_for_prediction = df[(i - datetime.timedelta(minutes=past_history+100)):i][-180:].values\n",
    "            past_for_prediction_encoded = std.transform(past_for_prediction.reshape(-1, 1))\n",
    "\n",
    "            # Prediction of A\n",
    "            results_A = LSTM_model_A.predict(past_for_prediction_encoded.reshape(1,past_history,1))[0]\n",
    "            results_A = std.inverse_transform(results_A)\n",
    "\n",
    "            # Prediction of B\n",
    "            results_B = LSTM_model_B.predict(past_for_prediction_encoded.reshape(1,past_history,1))[0]\n",
    "            results_B = std.inverse_transform(results_B)\n",
    "\n",
    "            df_results.prediction_A[i] = results_A\n",
    "            df_results.prediction_B[i] = results_B\n",
    "            df_results.real_values[i] = df[i: i + datetime.timedelta(minutes=60)][0:30].values\n",
    "\n",
    "            df_results.loc[i].metrics_A = measure_rmse(df_results.loc[i].real_values, df_results.loc[i].prediction_A)\n",
    "            df_results.loc[i].metrics_B = measure_rmse(df_results.loc[i].real_values, df_results.loc[i].prediction_B)\n",
    "        except:\n",
    "            print('error at', i)\n",
    "            \n",
    "            df_results.loc[i].metrics_A = None\n",
    "            df_results.loc[i].metrics_B = None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1001, 1002, 1003, 1006, 1007]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the list of the stations\n",
    "\n",
    "request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT station_id FROM velib_realtime\n",
    "\"\"\"\n",
    "df= request(query)\n",
    "# Removing bad values\n",
    "df= df.drop(0)\n",
    "df = df.drop(1391)\n",
    "list_of_stations = list(df.station_id)\n",
    "print(list_of_stations[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1390 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "#Variables\n",
    "\n",
    "day_of_testing = '2020-05-11'\n",
    "past_history = 180\n",
    "future_target = 30\n",
    "\n",
    "# Request for database\n",
    "\n",
    "for station_id in tqdm(list_of_stations):\n",
    "    \n",
    "    request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "    WHERE station_id = {}\n",
    "    AND date_of_update > DATE('2020-05-05')\n",
    "    AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "    ORDER BY date_of_update ASC\n",
    "    \"\"\".format(station_id, day_of_testing)\n",
    "\n",
    "    df = request(query)\n",
    "    df.index = df['date_of_update']\n",
    "    df = df.nb_total_free_bikes\n",
    "\n",
    "    df_results = pd.DataFrame(columns=['prediction_A', 'prediction_B', 'real_values', 'metrics_A', 'metrics_B'], index=pd.date_range(day_of_testing+' 06:00:00', periods=64, freq='15Min'))\n",
    "\n",
    "    # Training\n",
    "\n",
    "    LSTM_model_A, LSTM_model_A_history, LSTM_model_B, LSTM_model_B_history, std = model_training(station_id, day_of_testing, past_history, future_target)\n",
    "\n",
    "    # importing results\n",
    "    results_filling(df_results)\n",
    "    \n",
    "    df_results.to_csv(\"/home/exalis/Github/velib-prediction-v2/3. Results/2. Tensorflow Univariate/Tensorflow Univariate Results - {} - {}.csv\".format(day_of_testing, station_id))\n",
    "    \n",
    "    print('finished ', station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
