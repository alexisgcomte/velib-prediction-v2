{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensorflow Prophet Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of librairies\n",
    "import tensorflow as tf\n",
    "import mysql.connector as mariadb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "from joblib import dump\n",
    "\n",
    "class sql_query:\n",
    "    def __init__(self, credentials_path):\n",
    "        self.db_credentials = pd.read_csv(credentials_path, index_col=\"Field\")\n",
    "      \n",
    "    \n",
    "    def __call__(self, query):\n",
    "        \n",
    "        mariadb_connection = mariadb.connect(\n",
    "            user=self.db_credentials.loc[\"user\"][0],\n",
    "            password=self.db_credentials.loc[\"password\"][0],\n",
    "            host=self.db_credentials.loc[\"host\"][0],\n",
    "            port=3306,\n",
    "            db = \"db_velib\")\n",
    "        \n",
    "        self.cursor = mariadb_connection.cursor()\n",
    "    \n",
    "        cursor = self.cursor\n",
    "        cursor.execute(query)\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "        df = pd.DataFrame(cursor, columns=field_names)\n",
    "        return df\n",
    "    \n",
    "# Transforming the input data in the proper format \n",
    "\n",
    "\n",
    "\n",
    "def data_preparation(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "def measure_rmse(actual, predicted):\n",
    "    return math.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "def model_training(station_id, day_of_testing, past_history, future_target):\n",
    "\n",
    "\n",
    "    tf.random.set_seed(13)\n",
    "    past_history = 36\n",
    "    future_target = 6\n",
    "    STEP = 1\n",
    "    BATCH_SIZE = 32\n",
    "    BUFFER_SIZE = 100000\n",
    "    EPOCHS = 6\n",
    "    EVALUATION_INTERVAL = 200\n",
    "\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "    WHERE station_id = {}\n",
    "    AND date_of_update > DATE('2020-05-05')\n",
    "    AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "    AND MINUTE(date_of_update)%5=0\n",
    "    ORDER BY date_of_update ASC\n",
    "    \"\"\".format(station_id, day_of_testing)\n",
    "\n",
    "    df = request(query)\n",
    "    df.index = df['date_of_update']\n",
    "    df = df.nb_total_free_bikes\n",
    "    \n",
    "    \n",
    "    \n",
    "    TRAIN_SPLIT = round(df.shape[0]*0.7)\n",
    "\n",
    "    # StandardScaler transformation of the dataset\n",
    "\n",
    "    std = StandardScaler()\n",
    "    std.fit(df[:TRAIN_SPLIT].values.reshape(-1,1))\n",
    "    df = std.transform(df.values.reshape(-1,1))\n",
    "\n",
    "    # Creating proper format data\n",
    "\n",
    "    x_train, y_train = data_preparation(df, df[1:], 0, TRAIN_SPLIT,\n",
    "                                               past_history,\n",
    "                                               future_target, STEP)\n",
    "    x_val, y_val = data_preparation(df, df[1:], TRAIN_SPLIT, None,\n",
    "                                           past_history,\n",
    "                                           future_target, STEP)\n",
    "\n",
    "    # Creating format for NN intput\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "    x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)\n",
    "\n",
    "    # Creating batches for tensorflow use\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    val_data = val_data.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    # Modeling A\n",
    "    \n",
    "    LSTM_model_A = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(32, input_shape=x_train.shape[-2:]),\n",
    "        tf.keras.layers.Dense(future_target)\n",
    "    ])\n",
    "\n",
    "    LSTM_model_A.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    LSTM_model_A_history = LSTM_model_A.fit(train_data, epochs=EPOCHS,\n",
    "                                                steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                                validation_data=val_data,\n",
    "                                                validation_steps=200)\n",
    "    \n",
    "    # Modeling B\n",
    "\n",
    "    LSTM_model_B = keras.Sequential()\n",
    "    LSTM_model_B.add(\n",
    "      keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(\n",
    "          units=64,\n",
    "          input_shape=(x_train.shape[-2:])\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    LSTM_model_B.add(keras.layers.Dropout(rate=0.2))\n",
    "    LSTM_model_B.add(keras.layers.Dense(units=future_target))\n",
    "    LSTM_model_B.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    LSTM_model_B_history = LSTM_model_B.fit(train_data, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data,\n",
    "                                            validation_steps=200)\n",
    "    \n",
    "    \n",
    "    return LSTM_model_A, LSTM_model_A_history, LSTM_model_B, LSTM_model_B_history, std\n",
    "\n",
    "def results_filling(df_results):\n",
    "    \n",
    "    for i in df_results.index:\n",
    "        \n",
    "        try:\n",
    "            # interval // Taking the last 180 values\n",
    "            past_for_prediction = df[(i - datetime.timedelta(minutes=past_history*5+60)):i][-past_history:].values\n",
    "            past_for_prediction_encoded = std.transform(past_for_prediction.reshape(-1, 1))\n",
    "\n",
    "            # Prediction of A\n",
    "            results_A = LSTM_model_A.predict(past_for_prediction_encoded.reshape(1,past_history,1))[0]\n",
    "            results_A = std.inverse_transform(results_A)\n",
    "\n",
    "            # Prediction of B\n",
    "            results_B = LSTM_model_B.predict(past_for_prediction_encoded.reshape(1,past_history,1))[0]\n",
    "            results_B = std.inverse_transform(results_B)\n",
    "\n",
    "            df_results.prediction_A[i] = results_A\n",
    "            df_results.prediction_B[i] = results_B\n",
    "            df_results.real_values[i] = df[i: i + datetime.timedelta(minutes=60)][0:future_target].values\n",
    "\n",
    "            df_results.loc[i].metrics_A = measure_rmse(df_results.loc[i].real_values, df_results.loc[i].prediction_A)\n",
    "            df_results.loc[i].metrics_B = measure_rmse(df_results.loc[i].real_values, df_results.loc[i].prediction_B)\n",
    "        except:\n",
    "            print('error at', i)\n",
    "            \n",
    "            df_results.loc[i].metrics_A = None\n",
    "            df_results.loc[i].metrics_B = None\n",
    "        \n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1001, 1002, 1003, 1006, 1007]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the list of the stations\n",
    "\n",
    "request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT station_id FROM velib_realtime\n",
    "\"\"\"\n",
    "df= request(query)\n",
    "# Removing bad values\n",
    "df= df.drop(0)\n",
    "df = df.drop(1391)\n",
    "list_of_stations = list(df.station_id)\n",
    "print(list_of_stations[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/6\n",
      "200/200 [==============================] - 3s 15ms/step - loss: 0.3156 - val_loss: 0.8207\n",
      "Epoch 2/6\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1303 - val_loss: 0.6718\n",
      "Epoch 3/6\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1172 - val_loss: 0.6491\n",
      "Epoch 4/6\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1070 - val_loss: 0.6339\n",
      "Epoch 5/6\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0963 - val_loss: 0.6860\n",
      "Epoch 6/6\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.0990 - val_loss: 0.7143\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/6\n",
      "200/200 [==============================] - 4s 20ms/step - loss: 0.2567 - val_loss: 0.7912\n",
      "Epoch 2/6\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1278 - val_loss: 0.7534\n",
      "Epoch 3/6\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1223 - val_loss: 0.7117\n",
      "Epoch 4/6\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1168 - val_loss: 0.7591\n",
      "Epoch 5/6\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1008 - val_loss: 0.8099\n",
      "Epoch 6/6\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.0987 - val_loss: 0.8424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [00:25<00:25, 25.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished  1001\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/6\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.2598 - val_loss: 0.2933\n",
      "Epoch 2/6\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1284 - val_loss: 0.2568\n",
      "Epoch 3/6\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1226 - val_loss: 0.2614\n",
      "Epoch 4/6\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1162 - val_loss: 0.2695\n",
      "Epoch 5/6\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1078 - val_loss: 0.2583\n",
      "Epoch 6/6\n",
      "200/200 [==============================] - 1s 7ms/step - loss: 0.1134 - val_loss: 0.2555\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/6\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.2590 - val_loss: 0.3372\n",
      "Epoch 2/6\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1448 - val_loss: 0.2756\n",
      "Epoch 3/6\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1289 - val_loss: 0.2856\n",
      "Epoch 4/6\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1313 - val_loss: 0.2579\n",
      "Epoch 5/6\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1208 - val_loss: 0.2475\n",
      "Epoch 6/6\n",
      "200/200 [==============================] - 2s 10ms/step - loss: 0.1150 - val_loss: 0.2559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:50<00:00, 25.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished  1002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Main pipelinhe\n",
    "\n",
    "# Variables\n",
    "day_of_testing = '2020-05-17'\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "\n",
    "# Request for database\n",
    "\n",
    "for station_id in tqdm(list_of_stations):\n",
    "    \n",
    "    request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "    WHERE station_id = {}\n",
    "    AND date_of_update > DATE('2020-05-05')\n",
    "    AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "    ORDER BY date_of_update ASC\n",
    "    \"\"\".format(station_id, day_of_testing)\n",
    "\n",
    "    df = request(query)\n",
    "    df.index = df['date_of_update']\n",
    "    df = df.nb_total_free_bikes\n",
    "    \n",
    "\n",
    "\n",
    "    df_results = pd.DataFrame(columns=['prediction_A', 'prediction_B', 'real_values', 'metrics_A', 'metrics_B'], index=pd.date_range(day_of_testing+' 06:00:00', periods=64, freq='15Min'))\n",
    "\n",
    "    # Training\n",
    "\n",
    "    LSTM_model_A, LSTM_model_A_history, LSTM_model_B, LSTM_model_B_history, std = model_training(station_id, day_of_testing, past_history, future_target)\n",
    "\n",
    "    # Exporting results\n",
    "    \n",
    "    LSTM_model_A.save('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_A.h5'.format(day_of_testing, station_id))\n",
    "    LSTM_model_B.save('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_B.h5'.format(day_of_testing, station_id))\n",
    "    dump(std, '/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - std.joblib'.format(day_of_testing, station_id)) \n",
    "    \n",
    "   # df_results.to_csv(\"/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate Results - {} - {}.csv\".format(day_of_testing, station_id))\n",
    "    \n",
    "    \n",
    "    print('finished ', station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
