{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensorflow Prophet Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of librairies\n",
    "import tensorflow as tf\n",
    "import mysql.connector as mariadb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm import tqdm\n",
    "\n",
    "class sql_query:\n",
    "    def __init__(self, credentials_path):\n",
    "        self.db_credentials = pd.read_csv(credentials_path, index_col=\"Field\")\n",
    "      \n",
    "    \n",
    "    def __call__(self, query):\n",
    "        \n",
    "        mariadb_connection = mariadb.connect(\n",
    "            user=self.db_credentials.loc[\"user\"][0],\n",
    "            password=self.db_credentials.loc[\"password\"][0],\n",
    "            host=self.db_credentials.loc[\"host\"][0],\n",
    "            port=3306,\n",
    "            db = \"db_velib\")\n",
    "        \n",
    "        self.cursor = mariadb_connection.cursor()\n",
    "    \n",
    "        cursor = self.cursor\n",
    "        cursor.execute(query)\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "        df = pd.DataFrame(cursor, columns=field_names)\n",
    "        return df\n",
    "    \n",
    "# Transforming the input data in the proper format \n",
    "\n",
    "def data_preparation(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "def measure_rmse(actual, predicted):\n",
    "    return math.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "\n",
    "def model_training(station_id, day_of_testing, past_history, future_target):\n",
    "\n",
    "\n",
    "    tf.random.set_seed(13)\n",
    "    past_history = 180\n",
    "    future_target = 30\n",
    "    STEP = 1\n",
    "    BATCH_SIZE = 32\n",
    "    BUFFER_SIZE = 100000\n",
    "    EPOCHS = 15\n",
    "    EVALUATION_INTERVAL = 200\n",
    "\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "\n",
    "    # Taking data from  station 9034 - Madeleine\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "    WHERE station_id = {}\n",
    "    AND date_of_update > DATE('2020-05-05')\n",
    "    AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "    ORDER BY date_of_update ASC\n",
    "    \"\"\".format(station_id, day_of_testing)\n",
    "\n",
    "    df = request(query)\n",
    "    df.index = df['date_of_update']\n",
    "    df = df.nb_total_free_bikes\n",
    "    \n",
    "    TRAIN_SPLIT = round(df.shape[0]*0.7)\n",
    "\n",
    "    # StandardScaler transformation of the dataset\n",
    "\n",
    "    std = StandardScaler()\n",
    "    std.fit(df[:TRAIN_SPLIT].values.reshape(-1,1))\n",
    "    df = std.transform(df.values.reshape(-1,1))\n",
    "\n",
    "    # Creating proper format data\n",
    "\n",
    "    x_train, y_train = data_preparation(df, df[1:], 0, TRAIN_SPLIT,\n",
    "                                               past_history,\n",
    "                                               future_target, STEP)\n",
    "    x_val, y_val = data_preparation(df, df[1:], TRAIN_SPLIT, None,\n",
    "                                           past_history,\n",
    "                                           future_target, STEP)\n",
    "\n",
    "    # Creating format for NN intput\n",
    "\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "    x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)\n",
    "\n",
    "    # Creating batches for tensorflow use\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    val_data = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "    val_data = val_data.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "    # Modeling A\n",
    "    \n",
    "    LSTM_model_A = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(32, input_shape=x_train.shape[-2:]),\n",
    "        tf.keras.layers.Dense(future_target)\n",
    "    ])\n",
    "\n",
    "    LSTM_model_A.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    LSTM_model_A_history = LSTM_model_A.fit(train_data, epochs=EPOCHS,\n",
    "                                                steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                                validation_data=val_data,\n",
    "                                                validation_steps=200)\n",
    "    \n",
    "    # Modeling B\n",
    "    \n",
    "    LSTM_model_B = keras.Sequential()\n",
    "    LSTM_model_B.add(\n",
    "      keras.layers.Bidirectional(\n",
    "        keras.layers.LSTM(\n",
    "          units=64,\n",
    "          input_shape=(x_train.shape[-2:])\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    LSTM_model_B.add(keras.layers.Dropout(rate=0.2))\n",
    "    LSTM_model_B.add(keras.layers.Dense(units=30))\n",
    "    LSTM_model_B.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    LSTM_model_B_history = LSTM_model_B.fit(train_data, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data,\n",
    "                                            validation_steps=200)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return LSTM_model_A, LSTM_model_A_history, LSTM_model_B, LSTM_model_B_history, std\n",
    "\n",
    "def results_filling(df_results):\n",
    "\n",
    "    for i in df_results.index:\n",
    "        try:\n",
    "            # interval // Taking the last 180 values\n",
    "            past_for_prediction = df[(i - datetime.timedelta(minutes=past_history+100)):i][-180:].values\n",
    "            past_for_prediction_encoded = std.transform(past_for_prediction.reshape(-1, 1))\n",
    "\n",
    "            # Prediction of A\n",
    "            results_A = LSTM_model_A.predict(past_for_prediction_encoded.reshape(1,past_history,1))[0]\n",
    "            results_A = std.inverse_transform(results_A)\n",
    "\n",
    "            # Prediction of B\n",
    "            results_B = LSTM_model_B.predict(past_for_prediction_encoded.reshape(1,past_history,1))[0]\n",
    "            results_B = std.inverse_transform(results_B)\n",
    "\n",
    "            df_results.prediction_A[i] = results_A\n",
    "            df_results.prediction_B[i] = results_B\n",
    "            df_results.real_values[i] = df[i: i + datetime.timedelta(minutes=60)][0:30].values\n",
    "\n",
    "            df_results.loc[i].metrics_A = measure_rmse(df_results.loc[i].real_values, df_results.loc[i].prediction_A)\n",
    "            df_results.loc[i].metrics_B = measure_rmse(df_results.loc[i].real_values, df_results.loc[i].prediction_B)\n",
    "        except:\n",
    "            print('error at', i)\n",
    "            \n",
    "            df_results.loc[i].metrics_A = None\n",
    "            df_results.loc[i].metrics_B = None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1001, 1002, 1003, 1006, 1007]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the list of the stations\n",
    "\n",
    "request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT station_id FROM velib_realtime\n",
    "\"\"\"\n",
    "df= request(query)\n",
    "# Removing bad values\n",
    "df= df.drop(0)\n",
    "df = df.drop(1391)\n",
    "list_of_stations = list(df.station_id)\n",
    "print(list_of_stations[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1390 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/15\n",
      "200/200 [==============================] - 5s 23ms/step - loss: 0.2943 - val_loss: 1.0062\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0965 - val_loss: 0.8551\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0878 - val_loss: 0.7544\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0882 - val_loss: 0.7080\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0852 - val_loss: 0.6699\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0869 - val_loss: 0.6662\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0817 - val_loss: 0.6183\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0861 - val_loss: 0.6310\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0802 - val_loss: 0.5980\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0818 - val_loss: 0.5995\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0814 - val_loss: 0.5679\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0783 - val_loss: 0.5700\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0810 - val_loss: 0.5527\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0798 - val_loss: 0.6655\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0787 - val_loss: 0.5727\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/15\n",
      "200/200 [==============================] - 7s 33ms/step - loss: 0.2119 - val_loss: 0.7894\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.1057 - val_loss: 0.6051\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.1007 - val_loss: 0.5276\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0953 - val_loss: 0.4945\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0943 - val_loss: 0.3934\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0917 - val_loss: 0.3605\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0908 - val_loss: 0.3503\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0902 - val_loss: 0.3580\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0880 - val_loss: 0.3626\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0871 - val_loss: 0.3377\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0898 - val_loss: 0.3166\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0873 - val_loss: 0.2820\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0900 - val_loss: 0.3502\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0857 - val_loss: 0.2997\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0830 - val_loss: 0.3367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 1/1390 [01:53<43:41:40, 113.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished  1001\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/15\n",
      "200/200 [==============================] - 4s 19ms/step - loss: 0.2480 - val_loss: 0.4217\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0809 - val_loss: 0.3730\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0750 - val_loss: 0.3511\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0726 - val_loss: 0.3480\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0753 - val_loss: 0.3264\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0622 - val_loss: 0.3309\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0735 - val_loss: 0.3210\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0724 - val_loss: 0.3137\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 2s 12ms/step - loss: 0.0648 - val_loss: 0.3135\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0703 - val_loss: 0.3186\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0620 - val_loss: 0.3091\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0709 - val_loss: 0.3111\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0699 - val_loss: 0.3095\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0678 - val_loss: 0.3053\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0690 - val_loss: 0.3052\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/15\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.1787 - val_loss: 0.4023\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0891 - val_loss: 0.3575\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0722 - val_loss: 0.3314\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0904 - val_loss: 0.3207\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0746 - val_loss: 0.3212\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0729 - val_loss: 0.3278\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0733 - val_loss: 0.3144\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0717 - val_loss: 0.3201\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0835 - val_loss: 0.3090\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0751 - val_loss: 0.3126\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0709 - val_loss: 0.3121\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0677 - val_loss: 0.3152\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0768 - val_loss: 0.3080\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0691 - val_loss: 0.3075\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0734 - val_loss: 0.3048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 2/1390 [03:44<43:27:49, 112.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished  1002\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/15\n",
      "200/200 [==============================] - 4s 19ms/step - loss: 0.2408 - val_loss: 0.0895\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0638 - val_loss: 0.0772\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0598 - val_loss: 0.0751\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0570 - val_loss: 0.0752\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0540 - val_loss: 0.0734\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0573 - val_loss: 0.0737\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0535 - val_loss: 0.0741\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0511 - val_loss: 0.0729\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0550 - val_loss: 0.0706\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0507 - val_loss: 0.0723\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0503 - val_loss: 0.0720\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0545 - val_loss: 0.0713\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0509 - val_loss: 0.0712\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0529 - val_loss: 0.0696\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0502 - val_loss: 0.0707\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/15\n",
      "200/200 [==============================] - 6s 31ms/step - loss: 0.1663 - val_loss: 0.0784\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0716 - val_loss: 0.0759\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0649 - val_loss: 0.0733\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0650 - val_loss: 0.0737\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0617 - val_loss: 0.0729\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0578 - val_loss: 0.0718\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0622 - val_loss: 0.0726\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0558 - val_loss: 0.0721\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0603 - val_loss: 0.0710\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0619 - val_loss: 0.0747\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0534 - val_loss: 0.0729\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0595 - val_loss: 0.0717\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0548 - val_loss: 0.0722\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0593 - val_loss: 0.0703\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0567 - val_loss: 0.0712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 3/1390 [05:37<43:26:00, 112.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished  1003\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/15\n",
      "200/200 [==============================] - 4s 19ms/step - loss: 0.2485 - val_loss: 0.2487\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0783 - val_loss: 0.1553\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0777 - val_loss: 0.1148\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0746 - val_loss: 0.1182\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0718 - val_loss: 0.1003\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0702 - val_loss: 0.1037\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0699 - val_loss: 0.1054\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0767 - val_loss: 0.0963\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0702 - val_loss: 0.1066\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0679 - val_loss: 0.0942\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0761 - val_loss: 0.1010\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0712 - val_loss: 0.0947\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0737 - val_loss: 0.0999\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0718 - val_loss: 0.1079\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0701 - val_loss: 0.0921\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/15\n",
      "200/200 [==============================] - 6s 32ms/step - loss: 0.1776 - val_loss: 0.1241\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0860 - val_loss: 0.0838\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0885 - val_loss: 0.1071\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0727 - val_loss: 0.0888\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0815 - val_loss: 0.0713\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0786 - val_loss: 0.0778\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0746 - val_loss: 0.0719\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0729 - val_loss: 0.0782\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0725 - val_loss: 0.0774\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0766 - val_loss: 0.0794\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0719 - val_loss: 0.0733\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0678 - val_loss: 0.0705\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0699 - val_loss: 0.0724\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - 4s 22ms/step - loss: 0.0696 - val_loss: 0.0766\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.0705 - val_loss: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 4/1390 [07:29<43:19:15, 112.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished  1006\n",
      "Train for 200 steps, validate for 200 steps\n",
      "Epoch 1/15\n",
      "200/200 [==============================] - 4s 18ms/step - loss: 0.2345 - val_loss: 0.1595\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0594 - val_loss: 0.1358\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0581 - val_loss: 0.1315\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0555 - val_loss: 0.1293\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0514 - val_loss: 0.1298\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0552 - val_loss: 0.1290\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0509 - val_loss: 0.1261\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - 3s 13ms/step - loss: 0.0542 - val_loss: 0.1265\n",
      "Epoch 9/15\n",
      "114/200 [================>.............] - ETA: 0s - loss: 0.0493"
     ]
    }
   ],
   "source": [
    "#Variables\n",
    "\n",
    "day_of_testing = '2020-05-11'\n",
    "past_history = 180\n",
    "future_target = 30\n",
    "\n",
    "# Request for database\n",
    "\n",
    "for station_id in tqdm(list_of_stations):\n",
    "    \n",
    "    request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "    WHERE station_id = {}\n",
    "    AND date_of_update > DATE('2020-05-05')\n",
    "    AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "    ORDER BY date_of_update ASC\n",
    "    \"\"\".format(station_id, day_of_testing)\n",
    "\n",
    "    df = request(query)\n",
    "    df.index = df['date_of_update']\n",
    "    df = df.nb_total_free_bikes\n",
    "\n",
    "    df_results = pd.DataFrame(columns=['prediction_A', 'prediction_B', 'real_values', 'metrics_A', 'metrics_B'], index=pd.date_range(day_of_testing+' 06:00:00', periods=64, freq='15Min'))\n",
    "\n",
    "    # Training\n",
    "\n",
    "    LSTM_model_A, LSTM_model_A_history, LSTM_model_B, LSTM_model_B_history, std = model_training(station_id, day_of_testing, past_history, future_target)\n",
    "\n",
    "    # importing results\n",
    "    results_filling(df_results)\n",
    "    \n",
    "    df_results.to_csv(\"/home/exalis/Github/velib-prediction-v2/3. Results/2. Tensorflow Univariate/Tensorflow Univariate Results - {} - {}.csv\".format(day_of_testing, station_id))\n",
    "    \n",
    "    print('finished ', station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
