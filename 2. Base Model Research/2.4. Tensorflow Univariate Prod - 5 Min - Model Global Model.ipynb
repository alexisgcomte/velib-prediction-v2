{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensorflow Prophet Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of librairies\n",
    "import tensorflow as tf\n",
    "import mysql.connector as mariadb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "from joblib import dump\n",
    "\n",
    "class sql_query:\n",
    "    def __init__(self, credentials_path):\n",
    "        self.db_credentials = pd.read_csv(credentials_path, index_col=\"Field\")\n",
    "      \n",
    "    \n",
    "    def __call__(self, query):\n",
    "        \n",
    "        mariadb_connection = mariadb.connect(\n",
    "            user=self.db_credentials.loc[\"user\"][0],\n",
    "            password=self.db_credentials.loc[\"password\"][0],\n",
    "            host=self.db_credentials.loc[\"host\"][0],\n",
    "            port=3306,\n",
    "            db = \"db_velib\")\n",
    "        \n",
    "        self.cursor = mariadb_connection.cursor()\n",
    "    \n",
    "        cursor = self.cursor\n",
    "        cursor.execute(query)\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "        df = pd.DataFrame(cursor, columns=field_names)\n",
    "        return df\n",
    "    \n",
    "# Transforming the input data in the proper format \n",
    "\n",
    "\n",
    "\n",
    "def data_preparation(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "def measure_rmse(actual, predicted):\n",
    "    return math.sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipelinhe\n",
    "\n",
    "# Variables\n",
    "day_of_testing = '2020-05-19'\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "tf.random.set_seed(13)\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "STEP = 1\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 100000\n",
    "EPOCHS = 6\n",
    "EVALUATION_INTERVAL = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 100/1389 [00:40<08:48,  2.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4b15db8e14ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     x_train, y_train = data_preparation(df, df[1:], 0, TRAIN_SPLIT,\n\u001b[1;32m     72\u001b[0m                                                \u001b[0mpast_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                                                future_target, STEP)\n\u001b[0m\u001b[1;32m     74\u001b[0m     x_val, y_val = data_preparation(df, df[1:], TRAIN_SPLIT, None,\n\u001b[1;32m     75\u001b[0m                                            \u001b[0mpast_history\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-1ed336823db3>\u001b[0m in \u001b[0;36mdata_preparation\u001b[0;34m(dataset, target, start_index, end_index, history_size, target_size, step, single_step)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhistory_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msingle_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Extracting the list of the stations\n",
    "\n",
    "request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT station_id FROM velib_realtime\n",
    "\"\"\"\n",
    "df= request(query)\n",
    "# Removing bad values\n",
    "df= df.drop(0)\n",
    "df = df.drop(1391)\n",
    "list_of_stations = list(df.station_id)\n",
    "print(list_of_stations[0:5])\n",
    "\n",
    "\n",
    "# Initializing and building dataset\n",
    "\n",
    "request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "\n",
    "# Intialization\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "WHERE station_id = {}\n",
    "AND date_of_update > DATE('2020-05-05')\n",
    "AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "AND MINUTE(date_of_update)%5=0\n",
    "ORDER BY date_of_update ASC\n",
    "\"\"\".format(list_of_stations[0], day_of_testing)\n",
    "\n",
    "df = request(query)\n",
    "df.index = df['date_of_update']\n",
    "df = df.nb_total_free_bikes\n",
    "\n",
    "TRAIN_SPLIT = round(df.shape[0]*0.7)\n",
    "\n",
    "# StandardScaler transformation of the dataset\n",
    "\n",
    "std = StandardScaler()\n",
    "std.fit(df[:TRAIN_SPLIT].values.reshape(-1,1))\n",
    "df = std.transform(df.values.reshape(-1,1))\n",
    "\n",
    "# Creating proper format data\n",
    "\n",
    "x_train, y_train = data_preparation(df, df[1:], 0, TRAIN_SPLIT,\n",
    "                                           past_history,\n",
    "                                           future_target, STEP)\n",
    "x_val, y_val = data_preparation(df, df[1:], TRAIN_SPLIT, None,\n",
    "                                       past_history,\n",
    "                                       future_target, STEP)\n",
    "\n",
    "# Creating conso bases\n",
    "\n",
    "x_train_conso = x_train\n",
    "y_train_conso = y_train\n",
    "x_val_conso = x_val\n",
    "y_val_conso = y_val\n",
    "\n",
    "# Looping with station_id\n",
    "for station_id in tqdm(list_of_stations[1:]):\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "    WHERE station_id = {}\n",
    "    AND date_of_update > DATE('2020-05-05')\n",
    "    AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "    AND MINUTE(date_of_update)%5=0\n",
    "    ORDER BY date_of_update ASC\n",
    "    \"\"\".format(station_id, day_of_testing)\n",
    "\n",
    "    df = request(query)\n",
    "    df.index = df['date_of_update']\n",
    "    df = df.nb_total_free_bikes\n",
    "\n",
    "\n",
    "\n",
    "    TRAIN_SPLIT = round(df.shape[0]*0.7)\n",
    "\n",
    "    # StandardScaler transformation of the dataset\n",
    "\n",
    "    df = std.transform(df.values.reshape(-1,1))\n",
    "\n",
    "    # Creating proper format data\n",
    "\n",
    "    x_train, y_train = data_preparation(df, df[1:], 0, TRAIN_SPLIT,\n",
    "                                               past_history,\n",
    "                                               future_target, STEP)\n",
    "    x_val, y_val = data_preparation(df, df[1:], TRAIN_SPLIT, None,\n",
    "                                           past_history,\n",
    "                                           future_target, STEP)\n",
    "\n",
    "    # Creating format for NN intput\n",
    "    \n",
    "    x_train_conso = np.concatenate([x_train_conso, x_train])\n",
    "    y_train_conso = np.concatenate([y_train_conso, y_train])\n",
    "    x_val_conso = np.concatenate([x_val_conso, x_val])\n",
    "    y_val_conso =  np.concatenate([y_val_conso, y_val])\n",
    "    \n",
    "    #print('x_train_conso shape : ', x_train_conso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 20000 steps, validate for 200 steps\n",
      "Epoch 1/6\n",
      "20000/20000 [==============================] - 344s 17ms/step - loss: 0.1420 - val_loss: 0.3569\n",
      "Epoch 2/6\n",
      "20000/20000 [==============================] - 358s 18ms/step - loss: 0.1276 - val_loss: 0.3628\n",
      "Epoch 3/6\n",
      "20000/20000 [==============================] - 359s 18ms/step - loss: 0.1278 - val_loss: 0.3568\n",
      "Epoch 4/6\n",
      "20000/20000 [==============================] - 383s 19ms/step - loss: 0.1279 - val_loss: 0.3569\n",
      "Epoch 5/6\n",
      "20000/20000 [==============================] - 361s 18ms/step - loss: 0.1273 - val_loss: 0.3559\n",
      "Epoch 6/6\n",
      "20000/20000 [==============================] - 351s 18ms/step - loss: 0.1270 - val_loss: 0.3592\n",
      "Train for 20000 steps, validate for 200 steps\n",
      "Epoch 1/6\n",
      "20000/20000 [==============================] - 510s 26ms/step - loss: 0.1569 - val_loss: 0.3756\n",
      "Epoch 2/6\n",
      "20000/20000 [==============================] - 503s 25ms/step - loss: 0.1407 - val_loss: 0.3566\n",
      "Epoch 3/6\n",
      "20000/20000 [==============================] - 521s 26ms/step - loss: 0.1388 - val_loss: 0.3632\n",
      "Epoch 4/6\n",
      "20000/20000 [==============================] - 579s 29ms/step - loss: 0.1382 - val_loss: 0.3562\n",
      "Epoch 5/6\n",
      "20000/20000 [==============================] - 568s 28ms/step - loss: 0.1378 - val_loss: 0.3680\n",
      "Epoch 6/6\n",
      "20000/20000 [==============================] - 559s 28ms/step - loss: 0.1373 - val_loss: 0.3623\n"
     ]
    }
   ],
   "source": [
    "# Back to regular baseline\n",
    "\n",
    "x_train_conso = x_train_conso.reshape(x_train_conso.shape[0], x_train_conso.shape[1], 1)\n",
    "x_val_conso = x_val_conso.reshape(x_val_conso.shape[0], x_val_conso.shape[1], 1)\n",
    "\n",
    "\n",
    "# Creating batches for tensorflow use\n",
    "\n",
    "train_data_conso = tf.data.Dataset.from_tensor_slices((x_train_conso, y_train_conso))\n",
    "train_data_conso = train_data_conso.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_conso = tf.data.Dataset.from_tensor_slices((x_val_conso, y_val_conso))\n",
    "val_data_conso = val_data_conso.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "# Modeling A\n",
    "\n",
    "LSTM_model_A = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(32, input_shape=x_train_conso.shape[-2:]),\n",
    "    tf.keras.layers.Dense(future_target)\n",
    "])\n",
    "\n",
    "LSTM_model_A.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "LSTM_model_A_history = LSTM_model_A.fit(train_data_conso, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data_conso,\n",
    "                                            validation_steps=200)\n",
    "\n",
    "# Modeling B\n",
    "\n",
    "LSTM_model_B = keras.Sequential()\n",
    "LSTM_model_B.add(\n",
    "  keras.layers.Bidirectional(\n",
    "    keras.layers.LSTM(\n",
    "      units=64,\n",
    "      input_shape=(x_train_conso.shape[-2:])\n",
    "    )\n",
    "  )\n",
    ")\n",
    "LSTM_model_B.add(keras.layers.Dropout(rate=0.2))\n",
    "LSTM_model_B.add(keras.layers.Dense(units=future_target))\n",
    "\n",
    "LSTM_model_B.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "LSTM_model_B_history = LSTM_model_B.fit(train_data_conso, epochs=EPOCHS,\n",
    "                                        steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                        validation_data=val_data_conso,\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export of the models\n",
    "\n",
    "LSTM_model_A.save('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_A.h5'.format(day_of_testing, 'global'))\n",
    "LSTM_model_B.save('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_B.h5'.format(day_of_testing, 'global'))\n",
    "dump(std, '/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - std.joblib'.format(day_of_testing, 'global'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
