{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tensorflow Prophet Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of librairies\n",
    "import tensorflow as tf\n",
    "import mysql.connector as mariadb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "from joblib import dump\n",
    "\n",
    "class sql_query:\n",
    "    def __init__(self, credentials_path):\n",
    "        self.db_credentials = pd.read_csv(credentials_path, index_col=\"Field\")\n",
    "      \n",
    "    \n",
    "    def __call__(self, query):\n",
    "        \n",
    "        mariadb_connection = mariadb.connect(\n",
    "            user=self.db_credentials.loc[\"user\"][0],\n",
    "            password=self.db_credentials.loc[\"password\"][0],\n",
    "            host=self.db_credentials.loc[\"host\"][0],\n",
    "            port=3306,\n",
    "            db = \"db_velib\")\n",
    "        \n",
    "        self.cursor = mariadb_connection.cursor()\n",
    "    \n",
    "        cursor = self.cursor\n",
    "        cursor.execute(query)\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "        df = pd.DataFrame(cursor, columns=field_names)\n",
    "        return df\n",
    "    \n",
    "# Transforming the input data in the proper format \n",
    "\n",
    "\n",
    "\n",
    "def data_preparation(dataset, target, start_index, end_index, history_size,\n",
    "                      target_size, step, single_step=False):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    start_index = start_index + history_size\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_size\n",
    "\n",
    "    for i in range(start_index, end_index):\n",
    "        indices = range(i-history_size, i, step)\n",
    "        data.append(dataset[indices])\n",
    "\n",
    "        if single_step:\n",
    "            labels.append(target[i+target_size])\n",
    "        else:\n",
    "            labels.append(target[i:i+target_size])\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "\n",
    "def measure_rmse(actual, predicted):\n",
    "    return math.sqrt(mean_squared_error(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main pipelinhe\n",
    "\n",
    "# Variables\n",
    "day_of_testing = '2020-05-19'\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "tf.random.set_seed(13)\n",
    "past_history = 36\n",
    "future_target = 6\n",
    "STEP = 1\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 99999999\n",
    "EPOCHS = 40\n",
    "EVALUATION_INTERVAL = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1389/1389 [10:35<00:00,  2.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the list of the stations\n",
    "\n",
    "request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT station_id FROM velib_realtime\n",
    "\"\"\"\n",
    "df= request(query)\n",
    "# Removing bad values\n",
    "df= df.drop(0)\n",
    "df = df.drop(1391)\n",
    "list_of_stations = list(df.station_id)\n",
    "\n",
    "\n",
    "# Initializing and building dataset\n",
    "\n",
    "request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "\n",
    "# Intialization\n",
    "\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "WHERE station_id = {}\n",
    "AND date_of_update > DATE('2020-05-05')\n",
    "AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "AND MINUTE(date_of_update)%5=0\n",
    "ORDER BY date_of_update ASC\n",
    "\"\"\".format(list_of_stations[0], day_of_testing)\n",
    "\n",
    "df = request(query)\n",
    "df.index = df['date_of_update']\n",
    "df = df.nb_total_free_bikes\n",
    "\n",
    "TRAIN_SPLIT = round(df.shape[0]*0.7)\n",
    "\n",
    "# StandardScaler transformation of the dataset\n",
    "\n",
    "std = StandardScaler()\n",
    "std.fit(df[:TRAIN_SPLIT].values.reshape(-1,1))\n",
    "df = std.transform(df.values.reshape(-1,1))\n",
    "\n",
    "# Creating proper format data\n",
    "\n",
    "x_train, y_train = data_preparation(df, df[1:], 0, TRAIN_SPLIT,\n",
    "                                           past_history,\n",
    "                                           future_target, STEP)\n",
    "x_val, y_val = data_preparation(df, df[1:], TRAIN_SPLIT, None,\n",
    "                                       past_history,\n",
    "                                       future_target, STEP)\n",
    "\n",
    "# Creating conso bases\n",
    "\n",
    "x_train_conso = x_train\n",
    "y_train_conso = y_train\n",
    "x_val_conso = x_val\n",
    "y_val_conso = y_val\n",
    "\n",
    "# Looping with station_id\n",
    "for station_id in tqdm(list_of_stations[1:]):\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT date_of_update, nb_total_free_bikes FROM velib_realtime\n",
    "    WHERE station_id = {}\n",
    "    AND date_of_update > DATE('2020-05-05')\n",
    "    AND date_of_update <= DATE_ADD(DATE('{}'), INTERVAL 1 DAY)\n",
    "    AND MINUTE(date_of_update)%5=0\n",
    "    ORDER BY date_of_update ASC\n",
    "    \"\"\".format(station_id, day_of_testing)\n",
    "\n",
    "    df = request(query)\n",
    "    df.index = df['date_of_update']\n",
    "    df = df.nb_total_free_bikes\n",
    "\n",
    "\n",
    "\n",
    "    TRAIN_SPLIT = round(df.shape[0]*0.7)\n",
    "\n",
    "    # StandardScaler transformation of the dataset\n",
    "\n",
    "    df = std.transform(df.values.reshape(-1,1))\n",
    "\n",
    "    # Creating proper format data\n",
    "\n",
    "    x_train, y_train = data_preparation(df, df[1:], 0, TRAIN_SPLIT,\n",
    "                                               past_history,\n",
    "                                               future_target, STEP)\n",
    "    x_val, y_val = data_preparation(df, df[1:], TRAIN_SPLIT, None,\n",
    "                                           past_history,\n",
    "                                           future_target, STEP)\n",
    "\n",
    "    # Creating format for NN intput\n",
    "    \n",
    "    x_train_conso = np.concatenate([x_train_conso, x_train])\n",
    "    y_train_conso = np.concatenate([y_train_conso, y_train])\n",
    "    x_val_conso = np.concatenate([x_val_conso, x_val])\n",
    "    y_val_conso =  np.concatenate([y_val_conso, y_val])\n",
    "    \n",
    "    #print('x_train_conso shape : ', x_train_conso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_conso shape :  (4147760, 36, 1)\n"
     ]
    }
   ],
   "source": [
    "print('x_train_conso shape : ', x_train_conso.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 150 steps, validate for 200 steps\n",
      "Epoch 1/40\n",
      "150/150 [==============================] - 11s 74ms/step - loss: 2.9021 - val_loss: 2.2335\n",
      "Epoch 2/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.5227 - val_loss: 1.0720\n",
      "Epoch 3/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.2847 - val_loss: 0.7613\n",
      "Epoch 4/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.2962 - val_loss: 0.5396\n",
      "Epoch 5/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1866 - val_loss: 0.4590\n",
      "Epoch 6/40\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.1699 - val_loss: 0.4248\n",
      "Epoch 7/40\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.1604 - val_loss: 0.3979\n",
      "Epoch 8/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1697 - val_loss: 0.3826\n",
      "Epoch 9/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1352 - val_loss: 0.3726\n",
      "Epoch 10/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1462 - val_loss: 0.3684\n",
      "Epoch 11/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1290 - val_loss: 0.3656\n",
      "Epoch 12/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1429 - val_loss: 0.3634\n",
      "Epoch 13/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1422 - val_loss: 0.3650\n",
      "Epoch 14/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1191 - val_loss: 0.3619\n",
      "Epoch 15/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1454 - val_loss: 0.3612\n",
      "Epoch 16/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1216 - val_loss: 0.3585\n",
      "Epoch 17/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1200 - val_loss: 0.3592\n",
      "Epoch 18/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1293 - val_loss: 0.3585\n",
      "Epoch 19/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1218 - val_loss: 0.3669\n",
      "Epoch 20/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1205 - val_loss: 0.3604\n",
      "Epoch 21/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1121 - val_loss: 0.3599\n",
      "Epoch 22/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1180 - val_loss: 0.3591\n",
      "Epoch 23/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1296 - val_loss: 0.3612\n",
      "Epoch 24/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1190 - val_loss: 0.3601\n",
      "Epoch 25/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1234 - val_loss: 0.3600\n",
      "Epoch 26/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1214 - val_loss: 0.3589\n",
      "Epoch 27/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1258 - val_loss: 0.3591\n",
      "Epoch 28/40\n",
      "150/150 [==============================] - 1s 8ms/step - loss: 0.1282 - val_loss: 0.3602\n",
      "Epoch 29/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1245 - val_loss: 0.3583\n",
      "Epoch 30/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1272 - val_loss: 0.3572\n",
      "Epoch 31/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1155 - val_loss: 0.3680\n",
      "Epoch 32/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1249 - val_loss: 0.3619\n",
      "Epoch 33/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1208 - val_loss: 0.3732\n",
      "Epoch 34/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1169 - val_loss: 0.3609\n",
      "Epoch 35/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1101 - val_loss: 0.3690\n",
      "Epoch 36/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1452 - val_loss: 0.3667\n",
      "Epoch 37/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1138 - val_loss: 0.3590\n",
      "Epoch 38/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1140 - val_loss: 0.3654\n",
      "Epoch 39/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1259 - val_loss: 0.3668\n",
      "Epoch 40/40\n",
      "150/150 [==============================] - 1s 9ms/step - loss: 0.1191 - val_loss: 0.3612\n",
      "Train for 150 steps, validate for 150 steps\n",
      "Epoch 1/40\n",
      "150/150 [==============================] - 8s 54ms/step - loss: 1.6701 - val_loss: 1.1860\n",
      "Epoch 2/40\n",
      "150/150 [==============================] - 2s 12ms/step - loss: 0.3532 - val_loss: 0.7859\n",
      "Epoch 3/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.2774 - val_loss: 0.6669\n",
      "Epoch 4/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.2495 - val_loss: 0.5700\n",
      "Epoch 5/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.2398 - val_loss: 0.5516\n",
      "Epoch 6/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.2139 - val_loss: 0.5155\n",
      "Epoch 7/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.2014 - val_loss: 0.4840\n",
      "Epoch 8/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1988 - val_loss: 0.4714\n",
      "Epoch 9/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1953 - val_loss: 0.4576\n",
      "Epoch 10/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1838 - val_loss: 0.4367\n",
      "Epoch 11/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1864 - val_loss: 0.4411\n",
      "Epoch 12/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1947 - val_loss: 0.4305\n",
      "Epoch 13/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1770 - val_loss: 0.4349\n",
      "Epoch 14/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1700 - val_loss: 0.4416\n",
      "Epoch 15/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1805 - val_loss: 0.4430\n",
      "Epoch 16/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1749 - val_loss: 0.4228\n",
      "Epoch 17/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1745 - val_loss: 0.4182\n",
      "Epoch 18/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1556 - val_loss: 0.4100\n",
      "Epoch 19/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1640 - val_loss: 0.4191\n",
      "Epoch 20/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1617 - val_loss: 0.4190\n",
      "Epoch 21/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1564 - val_loss: 0.4182\n",
      "Epoch 22/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1597 - val_loss: 0.4023\n",
      "Epoch 23/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1673 - val_loss: 0.4807\n",
      "Epoch 24/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1643 - val_loss: 0.3949\n",
      "Epoch 25/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1545 - val_loss: 0.4096\n",
      "Epoch 26/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1523 - val_loss: 0.3966\n",
      "Epoch 27/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1519 - val_loss: 0.3879\n",
      "Epoch 28/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1498 - val_loss: 0.3912\n",
      "Epoch 29/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1420 - val_loss: 0.3891\n",
      "Epoch 30/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1607 - val_loss: 0.3898\n",
      "Epoch 31/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1486 - val_loss: 0.4155\n",
      "Epoch 32/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1473 - val_loss: 0.4156\n",
      "Epoch 33/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1568 - val_loss: 0.4022\n",
      "Epoch 34/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1598 - val_loss: 0.3874\n",
      "Epoch 35/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1624 - val_loss: 0.3979\n",
      "Epoch 36/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1426 - val_loss: 0.3848\n",
      "Epoch 37/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1519 - val_loss: 0.3890\n",
      "Epoch 38/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1495 - val_loss: 0.3883\n",
      "Epoch 39/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1358 - val_loss: 0.3925\n",
      "Epoch 40/40\n",
      "150/150 [==============================] - 2s 11ms/step - loss: 0.1482 - val_loss: 0.3937\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Back to regular baseline\n",
    "dump(std, '/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - std.joblib'.format(day_of_testing, 'global'))\n",
    "\n",
    "x_train_conso = x_train_conso.reshape(x_train_conso.shape[0], x_train_conso.shape[1], 1)\n",
    "x_val_conso = x_val_conso.reshape(x_val_conso.shape[0], x_val_conso.shape[1], 1)\n",
    "\n",
    "\n",
    "# Creating batches for tensorflow use\n",
    "\n",
    "train_data_conso = tf.data.Dataset.from_tensor_slices((x_train_conso, y_train_conso))\n",
    "train_data_conso = train_data_conso.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "val_data_conso = tf.data.Dataset.from_tensor_slices((x_val_conso, y_val_conso))\n",
    "val_data_conso = val_data_conso.batch(BATCH_SIZE).repeat()\n",
    "\n",
    "# Modeling A\n",
    "\n",
    "LSTM_model_A = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(32, input_shape=x_train_conso.shape[-2:]),\n",
    "    tf.keras.layers.Dense(future_target)\n",
    "])\n",
    "\n",
    "LSTM_model_A.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "LSTM_model_A_history = LSTM_model_A.fit(train_data_conso, epochs=EPOCHS,\n",
    "                                            steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                            validation_data=val_data_conso,\n",
    "                                            validation_steps=200)\n",
    "LSTM_model_A.save('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_A.h5'.format(day_of_testing, 'global {} val_steps'.format(STEP)))\n",
    "\n",
    "# Modeling B\n",
    "\n",
    "LSTM_model_B = keras.Sequential()\n",
    "LSTM_model_B.add(\n",
    "  keras.layers.Bidirectional(\n",
    "    keras.layers.LSTM(\n",
    "      units=64,\n",
    "      input_shape=(x_train_conso.shape[-2:])\n",
    "    )\n",
    "  )\n",
    ")\n",
    "LSTM_model_B.add(keras.layers.Dropout(rate=0.2))\n",
    "LSTM_model_B.add(keras.layers.Dense(units=future_target))\n",
    "\n",
    "LSTM_model_B.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "LSTM_model_B_history = LSTM_model_B.fit(train_data_conso, epochs=EPOCHS,\n",
    "                                        steps_per_epoch=EVALUATION_INTERVAL,\n",
    "                                        validation_data=val_data_conso,\n",
    "                                        validation_steps=EVALUATION_INTERVAL)\n",
    "\n",
    "LSTM_model_B.save('/home/exalis/Github/velib-prediction-v2/4. Models/Tensorflow Univariate - {} - {} - LSTM_B.h5'.format(day_of_testing, 'global {} val_steps'.format(EVALUATION_INTERVAL)))\n",
    "\n",
    "print('Finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
