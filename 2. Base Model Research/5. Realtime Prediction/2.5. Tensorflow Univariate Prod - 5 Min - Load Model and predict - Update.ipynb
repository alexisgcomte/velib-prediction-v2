{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import of librairies\n",
    "import tensorflow as tf\n",
    "import mysql.connector as mariadb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tqdm import tqdm\n",
    "from joblib import load\n",
    "from timeloop import Timeloop\n",
    "from datetime import timedelta\n",
    "import time \n",
    "\n",
    "class sql_query:\n",
    "    def __init__(self, credentials_path):\n",
    "        self.db_credentials = pd.read_csv(credentials_path, index_col=\"Field\")\n",
    "      \n",
    "    \n",
    "    def __call__(self, query):\n",
    "        \n",
    "        mariadb_connection = mariadb.connect(\n",
    "            user=self.db_credentials.loc[\"user\"][0],\n",
    "            password=self.db_credentials.loc[\"password\"][0],\n",
    "            host=self.db_credentials.loc[\"host\"][0],\n",
    "            port=3306,\n",
    "            db = \"db_velib\")\n",
    "        \n",
    "        self.cursor = mariadb_connection.cursor()\n",
    "        cursor = self.cursor\n",
    "        cursor.execute(\"SET  time_zone = 'Europe/Paris'\")\n",
    "        cursor.execute(query)\n",
    "        field_names = [i[0] for i in cursor.description]\n",
    "        df = pd.DataFrame(cursor, columns=field_names)\n",
    "        return df\n",
    "    \n",
    "# Transforming the input data in the proper format \n",
    "\n",
    "\n",
    "def measure_rmse(actual, predicted):\n",
    "    return math.sqrt(mean_squared_error(actual, predicted))\n",
    "\n",
    "def list_stations():\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "    query = \"\"\"\n",
    "    SELECT DISTINCT station_id FROM velib_realtime\n",
    "    \"\"\"\n",
    "    df= request(query)\n",
    "    # Removing bad values\n",
    "    df= df.drop(0)\n",
    "    df = df.drop(1391)\n",
    "    list_of_stations = list(df.station_id)\n",
    "    return list_of_stations\n",
    "\n",
    "def loading_models_unique(station_id, day_of_testing):\n",
    "\n",
    "    try:\n",
    "        LSTM_A = tf.keras.models.load_model('../4. Models/Tensorflow Univariate - {} - {} - LSTM_A.h5'.format(day_of_testing, station_id))\n",
    "        LSTM_B = tf.keras.models.load_model('../4. Models/Tensorflow Univariate - {} - {} - LSTM_B.h5'.format(day_of_testing, station_id))\n",
    "        std = load('../4. Models/Tensorflow Univariate - {} - {} - std.joblib'.format(day_of_testing, station_id))\n",
    "        return LSTM_A, LSTM_B, std\n",
    "    \n",
    "    except:\n",
    "        print('impossible to load ')\n",
    "\n",
    "\n",
    "def create_result_df(past_history):\n",
    "    # Extracting base for prediction \n",
    "\n",
    "    request = sql_query(\"../../aws_mariadb_crendentials.csv\")\n",
    "#\n",
    "#    query = \"\"\"\n",
    "#    SELECT station_id, date_of_update, nb_total_free_bikes FROM db_velib.velib_realtime\n",
    "#    WHERE date_of_update >= (SELECT * FROM (SELECT distinct date_of_update FROM db_velib.velib_realtime\n",
    "#    WHERE MINUTE(date_of_update)%5 = 0\n",
    "#    ORDER BY date_of_update DESC \n",
    "#    LIMIT 36) as temp\n",
    "#    ORDER BY date_of_update ASC \n",
    "#    LIMIT 1)\n",
    "#    AND MINUTE(date_of_update)%5 = 0\n",
    "#    ORDER BY station_id, date_of_update ASC;\n",
    "#    \"\"\"\n",
    "\n",
    "    query = \"\"\"\n",
    "    SELECT station_id, date_of_update, nb_total_free_bikes FROM db_velib.velib_realtime\n",
    "    WHERE date_of_update >= DATE_SUB(NOW(), INTERVAL 220 Minute) AND MINUTE(date_of_update)%5 = 0\n",
    "    ORDER BY station_id, date_of_update ASC;\n",
    "    \"\"\"\n",
    "    df = request(query)\n",
    "    # Selecting only the last 36 date_of_update values. We don't do it in SQL as it is longer!\n",
    "    df = df[df['date_of_update'].isin(list(df['date_of_update'].unique())[-past_history:])]\n",
    "    df.index = df.date_of_update\n",
    "    df = df[['station_id','nb_total_free_bikes']]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def predict_iteration_unique(list_of_stations, df, LSTM_A, LSTM_B, std):\n",
    "    # Request for each minutes\n",
    "    \n",
    "    df_prediction = pd.DataFrame(columns=['time', 'station_id','model_A', 'model_B'])\n",
    "\n",
    "    for station_id in tqdm(list_of_stations):\n",
    "        try:\n",
    "\n",
    "            df_prediction_temp = pd.DataFrame(columns=['time', 'station_id','model_A', 'model_B'])\n",
    "            df_prediction_temp[\"time\"] = list(pd.date_range(max(df.index), periods=7, freq='5Min'))[1:]\n",
    " \n",
    "            df_prediction_temp[\"station_id\"] = station_id\n",
    "\n",
    "            input_data = std.transform(df[df[\"station_id\"] == station_id].nb_total_free_bikes.values[:-36].reshape(-1, 1))\n",
    "            df_prediction_temp['model_A'] = std.inverse_transform(LSTM_A.predict(input_data.reshape(1,past_history,1))[0])\n",
    "            df_prediction_temp['model_B'] = std.inverse_transform(LSTM_B.predict(input_data.reshape(1,past_history,1))[0])\n",
    "            df_prediction = pd.concat([df_prediction, df_prediction_temp])\n",
    "        \n",
    "        except:\n",
    "            print('error on ', station_id)\n",
    "            \n",
    "    return df_prediction\n",
    "\n",
    "\n",
    "def predict_iteration_unique_2(df, LSTM_A, LSTM_B, std):\n",
    "    # Creating a template for an array the batch size\n",
    "    data = np.array(df.pivot_table(df, index= 'station_id', columns=df.index))\n",
    "\n",
    "    # Making nb of station x previous values shape\n",
    "    data = data.reshape(data.shape[0], data.shape[1])\n",
    "\n",
    "    # Standard Scaling\n",
    "    data = std.transform(data)\n",
    "\n",
    "    # Reshaping for tensor transformation and batchisation\n",
    "    data = data.reshape(data.shape[0], data.shape[1], 1)\n",
    "\n",
    "    # Making a prediction\n",
    "\n",
    "    pred_values_A = std.inverse_transform(LSTM_A.predict(data))\n",
    "    pred_values_B = std.inverse_transform(LSTM_B.predict(data))\n",
    "\n",
    "\n",
    "    df_pred_A = pd.DataFrame(pred_values_A, index=df.station_id.unique(), columns=list(pd.date_range(max(df.index), periods=7, freq='5Min'))[1:])\n",
    "    df_pred_A = df_pred_A.set_index(df_pred_A.index).stack().reset_index(name='model_A').rename(columns={'level_0':'station_id','level_1':'date'})\n",
    "\n",
    "    df_pred_B = pd.DataFrame(pred_values_B, index=df.station_id.unique(), columns=list(pd.date_range(max(df.index), periods=7, freq='5Min'))[1:])\n",
    "    df_pred_B = df_pred_B.set_index(df_pred_B.index).stack().reset_index(name='model_B').rename(columns={'level_0':'station_id','level_1':'date'})\n",
    "\n",
    "    # Merging\n",
    "\n",
    "    df_prediction = df_pred_A.merge(df_pred_B,left_on=[\"station_id\", \"date\"],right_on=[\"station_id\", \"date\"])\n",
    "    \n",
    "    return df_prediction\n",
    "            \n",
    "# Initialisation\n",
    "\n",
    "# Variables\n",
    "day_of_testing = '2020-05-19'\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
      "impossible to load \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "station_id = 'global'\n",
    "past_history = 36\n",
    "#future_target = 6\n",
    "#list_of_stations = list_stations() # inutile \n",
    "#BATCH_SIZE = 36\n",
    "\n",
    "# Loading the models only once\n",
    "LSTM_A, LSTM_B, std = loading_models_unique(station_id, day_of_testing)\n",
    "df = create_result_df(past_history)\n",
    "df_prediction = predict_iteration_unique_2(df, LSTM_A, LSTM_B, std)\n",
    "df_prediction['date_of_prediction'] = str(pd.Timestamp.now())[:16]\n",
    "df_prediction.to_csv('../7. Predictions/5min_predictions_global.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
